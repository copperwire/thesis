% !TEX spellckeck=en_GB

\section{Linear Regression}\label{sec:LinReg}

Modern machine learning has its foundations in the familiar framework of linear regression. Many fairly interesting problems can be cast as systems of linear problems, and as such there are multitudes of ways to solve the problem. The most straight forward of which is with just a little bit of linear algebra.

We begin by defining the constituents of our model: let our data be denoted as a matrix $\mathbf{X} \in \R^{m\times n+1}$ where $m$ is the number of data-points and $n$ the number of features. The $+1$ factor in the dimension is to accommodate the addition of a column of ones to our data as a convenient placeholder for the model intercept. Note also that the term features is used broadly in machine learning literature and denotes the measurable aspects of our system; in the 1D Ising model the $n$ would denote the number of spins and $m$ the number of measurements we made on that system. Furthermore let the parameter, or weight, matrix be given as $\mathbf{w} \in \R^{n + 1\times k}$. Generally the outcome-dimension $k$ can be greater than one when estimating multi-variate outcomes. For illustrative purposes we limit this section and the following to the case where $k=1$. The linear regression model is then the transformation of the input using the weight matrix, i.e. 

\begin{equation}\label{eq:og_linreg}
\mathbf{y} = \mathbf{X}\mathbf{w}.
\end{equation}
\noindent Finally let the outcome we wish to approximate be given as a vector $\mathbf{\hat{y}} \in \R^m$. We'll not concern ourselves overly with the properties of the process that generates $\mathbf{\hat{y}}$ in this section as this is elaborated on in the coming sections, and assume that that the outcome has no noise.

The challenge is then finding the weights that give correct predictions from data. To measure the quality of our model we introduce the squared distance between our predictions and $\mathbf{\hat{y}}$, which also gives us a path to optimization by the first derivative in terms of the model parameters. The squared error is defined in terms of the Euclidean vector norm  

$$L_2(\mathbf{x}) =||\mathbf{x}||_2 = \left(\sum x_i^2\right)^{\frac{1}{2}},$$

\noindent which we apply to the vector difference between model output and true values and take the square to give us a nice differentiable form. In mathematical terms we define the squared error objective as

\begin{equation}
\mathcal{O} = || \mathbf{\hat{y}} - \mathbf{y} ||_2 ^2.
\end{equation}

\noindent An objective function $\mathcal{O}$ defines some optimization problem to minimize or maximize. In machine learning these are mostly commonly cast as minimization problems. Objective functions for finding minima are termed cost functions in machine learning literature and we'll adopt that name moving forward. In this thesis we use the symbol $\mathcal{C}$ to indicate such functions and the optimization problem is then finding the minimum w.r.t the parameters $\theta^*$, i.e. 

\begin{align}\label{eq:cost}
\theta^* = \argmin_\theta \mathcal{C}(\mathbf{\hat{y}}, f(\mathbf{X}; \theta)).
\end{align}

\noindent We use the starred notation to indicate the optimal parameters for the given cost function.

Returning to the least squares problem the task is finding the optimal parameters now requires a differentiation, and to aid in that we write the objective as a matrix inner product 

\begin{align*}
\mathcal{C} &= || \mathbf{\hat{y}} - \mathbf{y} ||_2 ^2, \\
\mathcal{C} &= ( \mathbf{\hat{y}} - \mathbf{Xw})^T( \mathbf{\hat{y}} - \mathbf{Xw}).
\end{align*}

\noindent Since the problem is one of minimization we take the derivative with respect to the model parameters and locate its minima by setting it to zero 

\begin{align}
\nabla _{\mathbf{w}}\mathcal{C} &= \nabla _{\mathbf{w}} ( \mathbf{\hat{y}} - \mathbf{Xw})^T( \mathbf{\hat{y}} - \mathbf{Xw}), \\
&= -2\mathbf{X}^T\mathbf{\hat{y}} + 2\mathbf{X}^T\mathbf{X}\mathbf{w}, \\
\mathbf{0} &= -2\mathbf{X}^T\mathbf{\hat{y}} + 2\mathbf{X}^T\mathbf{X}\mathbf{w}, \\
\mathbf{X}^T\mathbf{\hat{y}} &= \mathbf{X}^T\mathbf{X}\mathbf{w}, \\
\mathbf{w} &=(\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T\mathbf{\hat{y}}. \label{eq:least_squares}
\end{align}

 \noindent This problem is analytically solvable with a plethora of tools, most notably we have the ones that don't perform the matrix inversion $(\mathbf{X}^T \mathbf{X})^{-1}$ as this inverse is not unique for data that does not have full rank.

Admittedly the least squares linear regression model is fairly simple and is rarely applicable to complex systems. Additionally, we've not discussed the assumptions made to ensure the validity of the model. Most important of which concerns the measurement errors, which we assume to be be identical independent normally distributed.

Given the relative simplicity of the linear regression model, and the fact that an analytical solution can be found, we'll use it for reference in understanding the coming sections in this chapter.




