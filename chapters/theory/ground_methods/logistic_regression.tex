% !TEX spellckeck=en_GB

\section{Logistic Regression}\label{sec:LogReg}

As mentioned previously a good portion of machine learning has the objective of identifying what class a given sample is drawn from. As a problem it has a very natural formation as classification is something we do both explicitly and implicitly every day. Visually identifying what animal the next-door neighbor is taking for a walk or when it is safe to cross the road are some classification tasks that we are very good at. In physics classification also holds significant interest. Whether it is identifying phase transitions from the state configuration of a thermodynamic system, or identifying reaction constituents from particle tracks which is the objective of this thesis.

To understand classification algorithms we begin from the simplest algorithm in classification; the perceptron \cite{Rosenblatt1958}. The perceptron uses the same transformation as in equation \ref{eq:og_linreg} and determines the class from the sign of the prediction. This is a rather crude representation of the problem and so we seek to refine it somewhat. The challenge lies in predicting a bounded variable like a probability $p \in [0,1]$ with a principally unbound transformation like in equation \ref{eq:og_linreg}. To construct a feasible model we begin by defining the odds of an event. The odds is simply defined as the ratio of probability for an event happening to the probability of it not happening, that is 

\begin{equation}\label{eq:odds}
o = \frac{p}{1-p}.
\end{equation}

\noindent Since $p$ is bounded in the unit interval unfortunately the odds are bounded in $\R^+$. Again the logarithm comes to the rescue as the logarithm of a positively bounded variable is unbounded, and so we define the log-odds as the output of our model given a data-point $\boldsymbol{x}_i$ and the model parameters $\boldsymbol{w}$

\begin{equation}\label{eq:log_odds}
\log \left(\frac{p_i}{1-p_i}\right) = \boldsymbol{x}_i\boldsymbol{w}. 
\end{equation} 

\noindent As a reminder we note that we add a column of ones to the data and thus include the intercept in the weights. 

We can then transform the log-odds back to a model for the probability which gives us the formulation for logistic regression 

\begin{equation}
q(y_i=1 | \boldsymbol{x}_i; \boldsymbol{w})= q(\boldsymbol{x}_i\boldsymbol{w}) = \frac{1}{1 + e^{-\boldsymbol{x}_i \boldsymbol{w}}}.
\end{equation} 

\noindent The term on the right-hand side is the logistic sigmoid function, which has the notable properties one needs from a function that models probabilities e.g. $f(x) = 1-f(1-x)$. With a binary outcome we can plug this model directly into the MLE cost defined in equation \ref{eq:mle}, i.e. 

\begin{align}\label{eq:mle_lr}
P(S|\boldsymbol{w}) &=  - \sum_i y_i\log q (\boldsymbol{x}_i\boldsymbol{w}) + (1-y_i)\log\left(1-q(\boldsymbol{x}_i\boldsymbol{w})\right).
\end{align}

\noindent Finding the optimal values for the parameters $\boldsymbol{w}$ is again a matter of finding the minimum of the cost. Noting first that the derivative of the logistic sigmoid is 

\begin{equation}
\nabla_{\boldsymbol{w}} q(\boldsymbol{x}_i \boldsymbol{w}) = q(\boldsymbol{x}_i \boldsymbol{w}) (1 - q(\boldsymbol{x}_i \boldsymbol{w}))\boldsymbol{x}_i,
\end{equation}

\noindent which is known as the sigmoid derivative identity in the machine learning literature. Additionally the derivative of the log model is straightforwardly computed as 

\begin{equation}
\nabla_{\boldsymbol{w}} \log q(\boldsymbol{x}_i \boldsymbol{w}) = 1- q(\boldsymbol{x}_i \boldsymbol{w})\boldsymbol{x}_i.
\end{equation}
 We can then write out the derivative of the cost as 

\begin{align}
\nabla_{\boldsymbol{w}} \mathcal{C} &= - \sum_i y_i (1 - q(\boldsymbol{x}_i \boldsymbol{w})) 1-y_i (-) q(\boldsymbol{x}_i \boldsymbol{w})\boldsymbol{x}_i, \\
&= - \sum_i \left(y_i -  q(\boldsymbol{x}_i \boldsymbol{w})\right)\boldsymbol{x}_i.
\end{align}

\noindent Unfortunately this derivative is transcendental in $\boldsymbol{w}$ which means that there is no closed form solution w.r.t. the parameters. Finding the optimal values for the parameters is then a problem that has to be solved with iterative methods. The same methods are used to fit very complex machine learning methods and as such proper understanding of these underpin the understanding of complex machine learning methods as much as understanding linear and logistic-regression. We discuss gradient descent in some detail in section \ref{sec:gd}, but first we re-visit linear regression with the MLE formalism fresh in mind.