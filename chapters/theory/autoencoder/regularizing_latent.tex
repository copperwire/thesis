\section{Regularizing Latent Spaces}\label{sec:latent}

As introduced in section \ref{sec:vae} the latent space of an autoencoder can be regularized to satisfy some distribution. The nature and objective of this regularization has been the subject of debate in machine learning literature since Kingma's original VAE paper in 2014. Two of the seminal papers published on the topic is the $\beta-VAE$ paper by \citet{Higgins2017} introducing a Lagrange multiplier to the traditional KL divergence term, and the Info-VAE paper by \citet{Zhao} criticizing the choice of a KL-divergence on the latent space. Where they further build on the $\beta-VAE$ proposition that the reconstruction and latent losses are not well balanced, and show that one can replace the KL-divergence term with another strict divergence and empirically show better results with these. In particular they show strong performance with a Maximum-Mean Discrepancy (MMD) divergence, which fits the moments of the latent distribution instead of measuring a point-wise divergence. By using any positive definite kernel $k(\cdot, \cdot)$\footnote{We will not probe deeply into the mathematics of kernel functions but they are used in machine learning most often for measuring distances, or applications in finding nearest neighbors. They are ordinarily proper metric functions. Some examples include the linear kernel: $k(x, x') = x^Tx'$ or the popular radial basis function kernel $k(x, x)=e^{-\frac{||x - x'||^2}{2\sigma}}$}  we define the MMD divergence as  

\begin{equation}\label{eq:mmd}
D_{MMD} = \langle k(z, z')\rangle_{p(z), p(z')} - 2 \langle k(z, z')\rangle_{q(z), p(z')} + \langle k(z, z')\rangle_{q(z), q(z')}.
\end{equation}

\noindent Which does not have a convenient closed form like the the Kullback-Leibler divergence and so adds some computational complexity. In this thesis we use mixture of Gaussian distributions of the prior $p(z)$ to encourage class separability in the latent space.

Recent research by \citet{Seybold2019}, amongst others, points to the challenge of the model collapsing to an autodecoder. In other words a sufficiently complex decoder can learn to reconstruct the sample independent of the latent sample \cite{Seybold2019}. To combat this problem they introduce a change in the optimization objective by adding a second decoder term to the optimization task

\begin{equation}\label{eq:duelling_decoder}
\langle\phi(x|z) + \lambda\phi'(x'|z) \rangle + \beta D(p || \psi).
\end{equation}

\noindent The second decoder term reconstructs a different representation of the sample $x$, and the change is dubbed as a duelling decoder model. In this work we will consider a duelling decoder that reconstructs a 2D charge projection reconstruction, which is the ordinary decoder, and a decoder that reconstructs a charge distribution or the measured net charge. As reactions happen and the charged particles move through the gas in the AT-TPC the amount of gas ionized varies and as such we expect that this second reconstruction objective will improve the amount of semantic information in the latent expressions.

