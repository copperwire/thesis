% !TEX spellckeck=en_GB


\section{Introduction to autoencoders}\label{sec:intro_autoenc}

The primary challenge we face with data from the AT-TPC (active target time projection chamber) is that labeling data is not always possible for a given experiment. Given that fact, and the huge amount of data available, we turn to dimensionality reduction methods to possibly express the different physics occurring in this data. One such set of methods is the autoencoder family of neural network algorithms.

An autoencoder is an attempt at learning a distribution of data by reconstruction. This means that the model encodes the data into a much lower dimensional latent space before projecting back into the original space of data. The goal, as a matter of course, is then to learn the true distribution, $P(\mathcal{X})$, over the data with some parametrized model $Q(\mathcal{X};\theta)$. The model consists of two discrete parts; an encoder and a decoder. The encoder is in general a non linear map $\psi$

\begin{align}
  \psi: \mathcal{X} \rightarrow \mathcal{Z},
\end{align}

\noindent where $\mathcal{X} $ and $\mathcal{Z}$ are spaces with $\text{dim}(\mathcal{X}) > \text{dim}(\mathcal{Z})$.
The second part of the model is the decoder that maps back to the original space

\begin{align}
  \phi: \mathcal{Z} \rightarrow \mathcal{X}.
\end{align}

\noindent The objective is then to find the configuration of the two maps $\phi$ and $\psi$ which gives the best possible reconstruction. That is the objective $\mathcal{O}$ for the model is given as

\begin{align}\label{eq:objective_autoenc}
  \mathcal{O} = \argmin_{\phi, \psi} \sum_i || \mathbf{x}_i - \phi(\psi( \mathbf{x}_i ))  ||^2
\end{align}

\noindent  As the name implies the encoder creates a lower-dimensional "encoded" representation of the input. This objective function is optimized by a mean-squared-error cost in the event of real valued data, but just as commonly through a binary cross-entropy for data normalized to the range $[0, 1]$. This representation can be useful for identifying whatever information-carrying variations are present in the data. This can be thought of as an analogue to PCA (principal component analysis)(\cite{Marsland2009}) which we introduced in chapter \ref{chap:fundament}. In practice the non-linear maps, $\psi$ and $\phi$, are most often parametrized by neural networks. We refer to section \ref{sec:ANN} for a detailed discussion on deep learning and neural networks.

The autoencoder has previously been successfully implemented in de-noising tasks. \todo{Citation needed} More recently the Machine Learning community discovered that one could impose a constraining term on the latent distribution to allow for the imposition of structure in the latent space. The goal in mind was to create a generative algorithm, a class of models used to sample from the distribution $P(\mathcal{X})$.

The first of these employed a Kullback-Leibler divergence and were dubbed variational autoencoders, or VAEs (\cite{Kingma2013}). While VAEs lost the contest for preeminence as a generative algorithm to adversarial\footnote{Adversarial networks are a pair of networks where one aims to generate realistic samples, and the other aims to separate between fake and real samples} networks, they remain a fixture in the literature of expressive latent variable models with development focusing on the salience of the latent space (\cite{Higgins2017}, \cite{Zhao}, \cite{Fertig}).

\section{The Variational Autoencoder}\label{sec:vae}

Originally presented by \citet{Kingma2013} the VAE is a twist upon the traditional autoencoder. Where the applications of an ordinary autoencoder largely extended to de-noising with some authors using it for dimensionality reduction, the VAE seeks to control the latent space of the model. The goal of the VAE is then to be able to generate samples from the unknown distribution over the data. In this thesis the generative properties of the algorithm is only interesting as a metric to describe the latent space. Our efforts largely concentrate on the latent space itself. Specifically the focus is discerning whether class membership, be it a physical property or something more abstract \footnote{examples include discerning whether a particle is  a proton or electron, or capturing the "five-ness" of a number in the handwritten digits MNIST dataset} is encoded.

We begin this chapter with the discussion of the VAE as the framework for deriving the cost and the methodology underpins the formalism on how we view the regularization of latent spaces.

\subsection{The variational autoencoder cost}

In section \ref{sec:intro_autoenc} we introduced the structure of the autoencoder. For the VAE, which is a more integral part of the technology used in the thesis, a more rigorous approach is warranted. We will here derive the loss function for the VAE in such a way that clarifies how we aim to impose known structure of the latent space.

We begin by considering the family of problems encountered in variational inference, where the VAE takes its theoretical inspiration. Specifically we will derive the VAE  as a solution to a Bayesian variational inference problem. Bayesian inference is a framework for linking some observed values $x$ to some hypothesized latent variable $z$. 

This family of techniques all begin with a consideration of the problem from Bayes' rule, which relates the probability of seeing our model given our data, $p(z|x)$ to the odds ratio of seeing our model $p(x|z)p(z)$ to the evidence $p(x)$. Bayes' rule can then be expressed mathematically as 

 \begin{equation}\label{eq:bayes}
 p(z| x) = \frac{p(x|z) p(z)}{p(x)}.
 \end{equation}

 \noindent The left hand side is termed the posterior distribution, which describes the updated knowledge given some prior belief expressed by the probability of the model and the likelihood which we know from chapter \ref{chap:fundament}. The last part of this equation is the denominator which is commonly referred to as the evidence. It is also what causes the problem to be challenging. To see why, we introduce some common re-writing tools used in statistical analysis. Beginning with the evidence which can be expressed as the integral of the joint distribution $p(x, z) = p(x|z)p(z)$ such that

\begin{equation}\label{eq:evidence}
p(x) = \int_z p(x|z)p(z)
\end{equation}

\noindent The integral in the denominator of equation \ref{eq:bayes} is intractable for most interesting problems, as the space over $z$ is usually combinatorially large. 

This is also the same problem that MCMC (Markov chain Monte Carlo) methods are commonly applied to. In physics this family of algorithms has been applied to solve many-body problems in quantum mechanics primarily by gradient descent on variational parameters \todo{citation-Comph-phys 2 compendium}.

We can then summarize variational Bayesian methods as being techniques for estimating computationally intractable integrals as the one expressed in \ref{eq:evidence}. To derive a solution we begin by introducing the KL-divergence (\cite{Kullback1951}), which is a measure of how much two distributions are alike. It is important to note that it is however not a metric. We define the KL-divergence in equation \ref{eq:kl} from a probability measure P, to another Q, by their probability density functions p, q over the set $x \in \mathcal{X}$

\begin{align}\label{eq:kl}
D_{KL} (P || Q) &= - \int^{\infty}_{-\infty} p(x) \log \left(\frac{p(x)}{q(x)}\right) dx, \\
&= \langle \log \left(\frac{p(x)}{q(x)} \right)\rangle_{p}.
\end{align}

\noindent In the context of the VAE the KL-divergence is a measure of the quality of P approximating Q (\cite{Burnham2002}). The first part of deriving the cost is then to introduce an approximation of the evidence. 

We also introduce the ELBO (evidence lower bound) as an approximation to the evidence following the derivation laid out by \cite{Kingma2013}. The EBLO function is defined as 

\begin{equation}\label{eq:elbo}
ELBO(q) = \langle \log(p(z, x)) \rangle - \langle \log(q(z|x)) \rangle.
\end{equation}

\noindent To fit the VAE cost we rewrite the ELBO in terms of the conditional distribution of $x$ given $z$

\begin{align}
ELBO(q) &= \langle \log(p(z)) \rangle +  \langle \log(p(x|z)) \rangle - \langle \log(q(z|x)) \rangle, \\
&=   \langle \log(p(x|z)) \rangle - D_{KL}(q(z|x) | p(z))
\end{align}

\noindent Finally the ELBO can be written as a lower bound on the evidence using Jensen's inequality (J) for concave functions. Mathematically we express the inequality as 

\begin{equation}
f(\langle x\rangle) \geq \langle f(x) \rangle.
\end{equation}

\noindent From the definition of the evidence we then have

\begin{align}
\log (p(x)) &= \log \int_z p(x|z) p(z), \\
&= log \int _z p(x|z) p(z) \frac{\psi(z|x)}{\psi(z|x)},\\
& = log \langle p(x|z) p(z)/ \psi (z|x) \rangle, \\
&  \stackrel{\mathclap{\text{(J)}}}{\geq} \langle \log (p(x|z) p(z)/ \psi(z|x))\rangle,\\
& = \langle \log(p(x|z))\rangle + \langle \log(p(z))\rangle  - \langle \log(\psi (z|x))\rangle, \\
\log (p (x)) &\geq \langle \log(p(x|z))\rangle - D_{KL}(\psi (z|x) || p(z)).
\end{align}

\noindent Showing that the ELBO is indeed a lower bound on the log evidence. 

Finally we move on to the VAE cost. We begin by defining $q(z|x)$ to be the posterior distribution over the latent variable $z \in \mathcal{Z}$, conditional on our data $x \in \mathcal{X}$ with evidence $p(x)$ and latent prior $q(z)$, with an associated probability measure $Q$ as per our notation above. Let then the parametrized distribution over the latent space enacted by the encoder be given as $\psi(z|x)$, and an associated probability measure $\Psi$. The quality of our encoder can then be decomposed as 

\begin{align}
D_{KL}(\Psi || Q ) &= \langle \log \left(\frac{\psi(z|x)}{q(z|x)}\right) \rangle_z, \\
&= \langle \log \left(\psi(z|x)- \log q(z|x)\right) \rangle_z.
\end{align}

\noindent From Bayes' rule we can re-state the posterior by introducing the decoder $\phi(x|z)$. Continuing the derivation we then have 

\begin{align}
D_{KL}(\Psi || Q ) &=  \langle \log \left( \psi(z|x)\right)  - \log \left( \phi( x | z) q(z) \right) + \log (q(x))\rangle_z, 
\end{align}
\noindent We identify that the evidence can be taken outside the expectation. Re-arranging the terms separates the model from the optimization targets 

\begin{align}
D_{KL}(\Psi || Q ) - \log q(x) &=  \langle \log \psi(z|x) - \log q(z) - \log \left( \phi( x | z)\right) \rangle_z, 
\end{align}

\noindent Note that the term $-\langle \log \left( \phi( x | z)\right) \rangle_z$ is the negative log likelihood of our decoder network which we can optimize with the cross entropy as discussed in section \ref{sec:LogReg}. We also identify that we can re-write the right side to include another KL divergence between the latent prior and the encoder. Flipping the sign then gives us the VAE cost

\begin{equation}\label{eq:vae_cost}
\log(p(x)) - D_{KL}(\Psi || Q )=  \langle \log \left( \phi( x | z)\right) \rangle_z - D_{KL}(\psi(z|x)|| q(z)).
\end{equation}
 
\noindent We are still bound by the intractable integral defining the evidence $p(x) = \int_z p(x, z)$ which is the same integral as in the denominator in equation \ref{eq:bayes}. This problem is solved by recognizing that the right hand side is the ELBO and so our model fits on the lower bound of the evidence.

\citet{Kingma2013} showed that this variational lower bound on the marginal likelihood of our data is feasibly approximated with a neural network when trained with backpropagation and gradient descent methods. That is we estimate the derivative of the ELBO with respect to the neural network parameters, as described by the backpropagation algorithm in section \ref{sec:backpropagation}.

\section{Optimizing the variational autoencoder}

From equation \ref{eq:vae_cost} we observe that the optimization is split in two. A reconstruction term that approximates the log evidence which we can train with a squared error or cross entropy cost. And secondly we have a divergence term over the parametrized and theoretical latent distribution. We would like to simplify the second to conserve computational resources. Thankfully this is simple given some assumptions on the target latent distribution. Let the target distribution $p(z | x) $ be a multivariate normal distribution with zero means and a diagonal unit variance matrix, i.e. $p(z | x) \sim  \mathcal{N}(\mathbf{0}, \mathbf{I})$. And accordingly the neural network approximation then follows $\psi(z | x) \sim \mathcal{N}(\mu, \Sigma)$. The normalized probability density function for the normal Gaussian is defined as 

\begin{equation}
p(\mathbf{x}) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(\mathbf{x} - \mathbb{\mu})^T\Sigma^{-1}(\mathbf{x}-\mu)),
\end{equation}

\noindent and the Kullback-Leibler divergence for two multivariate gaussians is given by 


\begin{equation}
\hspace*{-0.1in}
D_{KL}(p_1|| p_2 ) = \frac{1}{2} \left( \log \frac{|\Sigma_2|}{|\Sigma_1|} -n + tr(\Sigma^{-1}_2 \Sigma_1) + (\mu_2 - \mu_1)^T\Sigma_2^{-1}(\mu_2 - \mu_1) \right),
\end{equation}

\noindent which we derive in appendix \ref{appendix:kl_gauss}. Substituting $p_1$ and $p_2$ with the model distribution $\psi(z|x)$ and a latent prior $p(\mathbf{z}) \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ we get 

\begin{align*}
D_{KL}(q||p) &= \frac{1}{2} \left( - \log {|\Sigma_1|} -n + tr(I \Sigma_1) + \mu_2 ^TI\mu_2 \right) \\
&= \frac{1}{2} \left( - \log {|\Sigma_1|} -n + tr(\Sigma_1) + \mu_2 ^T\mu_2 \right),
\end{align*}

\noindent or more conveniently

\begin{equation}\label{eq:kl_opt}
D_{KL}(q||p) = \frac{1}{2} \sum_i -\log \sigma_i^2 - 1 + \sigma^2_i + \mu_i^2 .
\end{equation}


\noindent We note that equation \ref{eq:kl_opt} satisfies the equality that the divergence is zero when the target and model distributions are equal. An important feature of the Kullback-Leibler divergence is that it operates point-wise on the probability densities. Which was the topic of the argument presented by \citet{Zhao} when proposing alternate measures for regularizing the latent space. The intuitive alternative to a point-wise measurement is comparing the moments of the distribution and minimize their difference. 

\subsection{Mode collapse}\label{sec:mode_collapse}

When training a VAE we are balancing the evidence and prior latent distribution loss terms. \citet{Kingma2013} note that equation \ref{eq:vae_cost} is unbalanced in favor of the latent space regularization. As a consequence the un-weighted loss suffers from mode collapse. When the reconstruction is under-valued in the optimization the model will quickly learn the prior, without encoding the input. As the prior has become uninformative the decoder proceeds to learn a rough representation that covers as many of the outputs as possible. This can often visually be identified by the output being a vague cloud with some distinct regions. 