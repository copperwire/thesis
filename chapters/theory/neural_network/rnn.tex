% !TEX spellckeck=en_GB

\section{Recurrent Neural Networks}\label{sec:rnn}

The recurrent neural network (RNN) models a unit that has "memory". The memory is encoded as a state variable, and is ordinarily concatenated with the input to make a prediction. The model predictions typically enact a sequence which has led to applications text generation, time-series predictions, and other serialized applications. RNNs were first discussed in a theoretical paper by Jordan, MI in 86' but implemented in the modern temporal sense by \citet{Pearlmutter1989}. A simple graphical representation of the RNN cell is presented in figure \ref{fig:rnn}

\begin{figure}[h]
\centering
\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
\input{rnn.tikz}
\caption[Recurrent neural network cell]{A graphical illustration of the RNN cell. The self-connected edge in the left-hand side denotes the temporal nature we unroll on the right side. The cell takes as input a state vector and an input vector at time t and outputs a prediction and the new state vector used for the next prediction. Internally the simplest form this operation takes is to concatenate the state vector with the input and use an ordinary dense network as described in section \ref{sec:ANN} trained with back-propagation.}\label{fig:rnn}
\end{figure}

The memory encoded by the RNN cell is encoded as a state variable. Figure \ref{fig:rnn} gives a good intuition for an RNN cell, but we will elaborate on this by introducing the surprisingly simple forward pass structure for ordinary RNN cells. Let $X_t$ be the input to the RNN cell at time-step from zero to $n$, $\{0 \leq t \leq n: t \in \mathcal{Z} \}$ and $h_t$ be the state of the RNN cell at time $t$. Let also $y_t$ be the output of the RNN at time $t$. The nature of $X$ and $y$ are problem-specific, but common uses of these networks have been the prediction of words in a sentence. Where $X$ would represent the previous word in the sentence and $y$ the prediction over the set of available words for which comes next. An RNN cell can then be concisely formulated as

\begin{align}\label{eq:rnn}
\inner{[X_t, h_t]}{W} + b = h_{t+1},
\end{align}

\noindent where the weight matrix $W$ and bias vector $b$ are defined in the usual manner. Looking back at figure \ref{fig:rnn} the output should be a vector in $y$ space, and yet we have noted the output, $h_t$, as being in the state space of the cell. This notational difference is simply a convenience lending flexibility to our implementation, $h_t$ is transformed to the $y$ space by use of a regular fully connected layer. This transformation is a common trick in machine learning: leaving the inner parts of the algorithm extremely problem agnostic and using end-point layers to fit the problem at hand. To further clarify, we show the forward pass for a simple one-cell RNN in algorithm \ref{algo:rnn}. Expanding from a single cell to a model of complex systems, one can use the output from one RNN cell, $h_{t+1}$, as the input to another producing a deep recurrent network. 

\begin{algorithm}[H]
\SetAlgoLined
\caption{Defining the forward pass of a simple one cell RNN network. The cell accepts the previous state and corresponding data-point as input. These are batched vectors both, and so one usually concatenates the vectors along the feature axis to save time when doing the matrix multiplication. The cell maintains a weight matrix, $\boldsymbol{W}$, and bias, $b$, which will be updated by back-propagation of errors in the standard way.}\label{algo:rnn}
\KwResult{$\boldsymbol{h}_{t+1}$}
\KwIn{$\boldsymbol{h}_t$, $\boldsymbol{X}_t$}
\KwData{$\boldsymbol{W}$, b}
$\boldsymbol{F} \gets$ \KwSty{concatenate}(($h_t$, $\boldsymbol{X}_t$), axis$=1$)\;
$\boldsymbol{h}_{t+1}$ $\gets$ \KwSty{matmul}($\boldsymbol{F}$, $\boldsymbol{W}$) + b\;
\KwRet{$\boldsymbol{h}_{t+1}$}
\end{algorithm}

\noindent When designing a neural network, the researcher principally has five basic choices regarding the sequentiality of the model. We represent these five in figure \ref{fig:ann_architectures}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../figures/ann_types}
\caption[Archetypes of recurrent neural architectures]{The advent of recurrent networks enabled machine learning researchers to both model complex sequential behaviours like understanding patterns in a corpus of texts as well as using sequences to predict a single multivariate outcome and more. The leftmost figure \textbf{1)} represents an ordinary neural network, where the rectangles are matrix objects, red for input, blue for output and green for intermediary representations and the arrows are matrix operations like concatenation, multiplication and other arithmetic. \textbf{2)} shows a recurrent architecture for sequence output e.g. image captioning. Where the information about the previous word gets passed along forward by the state of the previous cell. \textbf{3)} transforming a sequence of observations to a single multivariate outcome. The classic example of which is sentiment prediction from text data. \textbf{4), 5)} Sequenced input and output can either be aligned as in the latter or misaligned as in the former. An example of a synced sequence to sequence can be phase prediction from a time series of a thermodynamic system. Un-synced applications include machine translation where sentences are processed then output in another language. Figure copied from \cite{Karpathy2015}}\label{fig:ann_architectures}
\end{figure}

\subsection{Long short-term memory cells}\label{sec:lstm}

One of the principal challenges for the RNN cell is in the handling of the state of the cell. The state acts as a sort of memory, but without moderation, it tries to recall the entire history of the sequence. \citet{Hochreiter1997} proposed a solution to this problem in their description of the long short-term memory (LSTM) network. The LSTM network introduced the ability for the network to selectively forget part of its memory. Being able to forget parts of the history was a boon to the language processing community, as they regularly tackle problems with both long and short term relationships. 

The LSTM network implements a series of layers inside each cell. By combining sigmoid, $\sigma{\cdot}$, and the hyperbolic tangent function, $\tau(\cdot)$, they compute point-wise manipulations of the state $\boldsymbol{h}_t$, previous output $\boldsymbol{c}_t$ and input $\boldsymbol{x}_t$. Lastly, we define the input gate $\boldsymbol{i}_t$, output gate $\boldsymbol{o}_t$ and forget gate $\boldsymbol{f}_t$, each with their own weight matrices $\boldsymbol{W}_{\{i, f, o, c\}}$. The gates are computed as

\begin{align}
\boldsymbol{i_t} = \sigma(\boldsymbol{W}_i[\boldsymbol{h}_{t-1}, \boldsymbol{x}_t]),\\
\boldsymbol{f_t} = \sigma(\boldsymbol{W}_f[\boldsymbol{h}_{t-1}, \boldsymbol{x}_t]),\\
\boldsymbol{o_t} = \sigma(\boldsymbol{W}_o[\boldsymbol{h}_{t-1}, \boldsymbol{x}_t]).
\end{align}

\noindent Following from these definitions, we can compute the new state and cell outputs

\begin{align}
\hat{\boldsymbol{c}}_t &= \tau (\boldsymbol{W}_o[\boldsymbol{h}_{t-1}, \boldsymbol{x}_t]), \\
\boldsymbol{c}_t &= \boldsymbol{i}_t \circ \hat{\boldsymbol{c}}_t + \boldsymbol{f}_t \circ \boldsymbol{c}_{t-1}, \\ 
\boldsymbol{h}_t &= \boldsymbol{o}_t \circ \tau (\boldsymbol{c}_t).
\end{align}

\noindent Recall the Hadamard product denoted by $\circ$, and the concatenation of two vectors denoted by square brackets. In the equations above, we have also followed the convention of including a column of ones in the data. This inclusion bakes the biases into the weight matrices, and is convenient for notational clarity. 

The LSTM network was designed to allow for the network to forget parts of the input. However, it also solves a vanishing gradient problem as the gradient flows through the network by the cell outputs.
