% !TEX spellckeck=en_GB

\section{Recurrent Neural Networks}\label{sec:rnn}

\subsection{Introduction to recurrent neural networks}

The recurrent neural network (RNN) models a unit that has "memory". The memory is encoded as a state variable, which is ordinarily concatenated with the input to make a prediction. The model predictions typically enact a sequence which has led to applications text generation, time series predictions, and other serialized applications. RNNs were first discussed in a theoretical paper by Jordan, MI in 86' but implemented in the modern temporal sense by \citet{Pearlmutter1989}. A simple graphical representation of the RNN cell is presented in figure \ref{fig:rnn}

\begin{figure}[h]
\centering
\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
\input{rnn.tikz}
\caption[Recurrent neural network cell]{A graphical illustration of the RNN cell. The self-connected edge in the left hand side denotes the temporal nature we unroll on the right side. The cell takes as input a state vector and an input vector at time t, and outputs a prediction and the new state vector used for the next prediction. Internally the simplest form this operation takes is to concatenate the state vector with the input and use an ordinary dense network as described in section \ref{sec:ANN} trained with back-propagation.}\label{fig:rnn}
\end{figure}

The memory encoded by the RNN cell is encoded as a state variable. Figure \ref{fig:rnn} gives a good intuition for a RNN cell, but we will elaborate on this by introducing the surprisingly simple forward pass structure for ordinary RNN cells. Let $X_t$ be the input to the RNN cell at time-step from zero to $n$, $\{0 \leq t \leq n: t \in \mathcal{Z} \}$ and $h_t$ be the state of the RNN cell at time $t$. Let also $y_t$ be the output of the RNN at time $t$. The nature of $X$ and $y$ are problem specific but a common use of these network has been the prediction of words in a sentence, such that $X$ is a representation of the previous word in the sentence and $y$ the prediction over the set of available words for which comes next. Our cell can then be simply formulated as in equation \ref{eq:rnn}.

\begin{align}\label{eq:rnn}
\inner{[X_t, h_t]}{W} + b = h_{t+1}
\end{align}

\noindent Where the weight matrix $W$ and bias vector $b$ are defined in the usual manner. Looking back at figure \ref{fig:rnn} the output should be a vector in $y$ space and yet we've noted the output as being in the state space of the cell. This is simply a convenience lending flexibility to our implementation, the new state is produced by the cell and transformed to the $y$ space by use of a normal linear fully connected layer. This is a common trick in machine learning: leaving the inner parts of the algorithm extremely problem agnostic and using end-point layers to fit the problem at hand. To further clarify, we show the forward pass for a simple one-cell RNN in algorithm \ref{algo:rnn}. The forward pass is remarkably simple and flexible all the same. To model complex systems one can use the output from one RNN cell, $h_{t+1}$, as the input to another RNN cell that maintains its own state. 

\begin{algorithm}[H]
\SetAlgoLined
\caption{Defining the forward pass of a simple one cell RNN network. The cell accepts the previous state and corresponding data-point as input. These are batched vectors both, and so one usually concatenates the vectors along the feature axis to save time when doing the matrix multiplication. The cell maintains a weight matrix, $\mathbf{W}$, and bias, $b$, which will be updated by back-propagation of errors in the standard way.}\label{algo:rnn}
\KwResult{$\mathbf{h}_{t+1}$}
\KwIn{$\mathbf{h}_t$, $\mathbf{X}_t$}
\KwData{$\mathbf{W}$, b}
$\mathbf{F} \gets$ \KwSty{concatenate}(($h_t$, $\mathbf{X}_t$), axis$=1$)\;
$\mathbf{h}_{t+1}$ $\gets$ \KwSty{matmul}($\mathbf{F}$, $\mathbf{W}$) + b\;
\KwRet{$\mathbf{h}_{t+1}$}
\end{algorithm}

\noindent Recurrent architectures present the researcher with a set of tools to not only model sequences, but to use a sequential structure to avoid common problems with e.g. variational autoencoders. \citet{Gregor2015} uses this aspect of recurrent networks in their DRAW (deep recurrent attentive writer) algorithm, which sequentially draws on a canvas to create realistic looking output images. In a non-sequential autoencoder we encounter the challenge that a given pixels activation is not conditioned on it's neighbors activation. In part this problem is what gives rise to the blurriness observed in the output from variational autoencoders. When designing a neural network the researcher principally has five basic choices regarding the sequentiality of the model. We represent these five in figure \ref{fig:ann_architectures}, which is copied from \citet{Karpathy2015}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../figures/ann_types}
\caption[Archetypes of recurrent neural architectures]{The advent of recurrent networks enabled machine learning researchers to both model complex sequential behaviors like understanding patterns in text as well as using sequences to predict a single multivariate outcome and more. The leftmost figure \textbf{1)} represents an ordinary neural network, where the rectangles are matrix objects, red for input, blue for output and green for intermediary representations and the arrows are matrix operations like concatenation multiplication etc. \textbf{2)} shows a recurrent architecture for sequence output e.g. image captioning. Where the information about the previous word gets passed along forward by the state of the previous cell. \textbf{3)} transforming a sequence of observations to a single multivariate outcome. The classical example of which is sentiment prediction from text. \textbf{4), 5)} Sequenced input and output can either be aligned as in the latter or misaligned as in the former. An example of synced sequence to sequence can be phase prediction from a time series of a thermodynamic system. Un-synced applications include machine translation where sentences are processed then output in another language. Figure copied from \cite{Karpathy2015}}\label{fig:ann_architectures}
\end{figure}

\subsection{Long short-term memory cells}\label{sec:lstm}

\begin{itemize}
\item natural extension of RNN, where RNN carries the entire long-term dependency LSTMs introduce forgetting parts of the history
\item implements gates in the architecture using sigmoid activations 
\end{itemize}