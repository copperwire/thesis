\subsection{Convolutional Neural Networks}\label{sec:cnn}

Equipped with an understanding of the constituent parts of a fully connected neural network as described in section \ref{sec:ANN}, we can now introduce different transformations used in modern neural networks. We begin this discussion with the introduction of the convolutional layer. Originally designed for image analysis tasks convolutional networks are now ubiquitous tools for sequence analysis, images, and image-like data. 

Data from the active target time projection chamber (AT-TPC), can be represented in a two-dimensional projection. While these projections do not exhibit all the same properties as traditional images, the analysis tools used for images are still applicable as shown by \citet{Kuchera2019}. The primary differences owe to the fact that the AT-TPC data lacks invariance under translation, rotation, and scale: Simply because the physics would change under variations of these properties. However, invariance under these properties is commonly applied in image analysis to improve generalization. Researchers typically employ tools such as translating objects or zooming to increase the amount of data available artificially.

We begin our discussion of convolutional networks with a comparison to ordinary fully connected, or \textit{dense}, networks introduced previously in this chapter.

There are a couple of challenges with neural network layers as they were introduced in section \ref{sec:ANN}. Firstly, the number of parameters can quickly get out of hand, as the number of parameters in a dense layer is the product of the input and output nodes. For example, a layer with $10^3$ inputs and $10^2$ nodes contains $10^5$ parameters. Another challenge the dense layer faces is the local structure in the data. As convolutional layers were developed primarily for images, the forward pass is constructed in a way that captures local structures in an efficient manner. In short, the advantage of convolutional layers is an allowance for a vastly reduced number of parameters at the cost of much higher demands of memory. The increased memory cost comes from the fact that convolutional networks make many transformations of the input at each layer that must be stored.

The increased memory cost comes from how we construct the convolutional layer. Each layer maintains $k$ filters, or kernels, each of which is a $n\times m$ matrix of weights. To compute the forward pass a stack of $k$ filters, $\mathbf{F}$, is convolved with the input by taking the inner product with a sliding $n\times m$ patch of the image thus iteratively moving over the entire input, $\mathbf{I}$ with size $h \times w \times c$. Mathematically we express the forward pass with the convolution operator $*$, and it can then be written in terms of an element of the output as

\begin{equation}\label{eq:conv}
(\mathbf{F}*\mathbf{I})_{ijk} = 
\sum_{f=-n'}^{n'}\sum_{g=-m'}^{m'}\sum_{h=1} ^c
I_{i+f,\, j+g,\, h}\cdot F_{fghk}.
\end{equation}

\noindent We iterate over the primed filter dimensions $n'=\lfloor\frac{n}{2} \rfloor$ and $m' = \lfloor \frac{m}{2} \rfloor$ in place of the non-primed dimensions to correctly align the input with the center element of the kernels. For this reason $n$ and $m$ are usually chosen to be odd integers. Equation \ref{eq:conv} is illustrated for $c=1$ in figure \ref{fig:conv_aritmetic}   

The convolution is computed over the entire depth of the input, i.e. along the channels of the image. Thus the kernel maintains a $n\times m$ matrix of weights for each layer of depth in the previous layer. For a square kernel of size $K$ that moves one pixel from left to right per step over a square image of size $W$ the output is then a square matrix with size $O$, i.e.

\begin{equation}\label{eq:simple_conv_out}
O = W - K +1.
\end{equation}

\noindent In practice, it is often beneficial to pad the image with one or more columns/rows of zeros such that the kernel fully covers the input. Additionally, one can down-sample by moving the kernel more than one pixel at a time. This is called the stride of the layer and has a very visually intuitive representation that is illustrated in figure \ref{fig:conv_aritmetic}. The full computation of the down-sizing with these effects then is a modified version of equation \ref{eq:simple_conv_out}, namely: 

\begin{equation}\label{eq:conv_out}
O = \frac{W - K + 2P}{S} + 1.
\end{equation} 

\noindent The modification includes the addition of an additive term from the padding, $P$, and a division by the stride (i.e., how many pixels the kernel jumps each step), $S$. Striding provides a convenient way to down-sample the input, which lessens the memory needed to train the model. Traditionally MaxPooling has also been used to achieve the same result. MaxPooling is a naive down-sampling algorithm that selects the highest value from the set of disjoint $m\times m$ patches of the input, where $m$ is the pooling number. In practice, $m=2$ has been the most common value for MaxPooling as it results in a halving of the input in both width and height.

\begin{figure}
\centering
\subfloat[{}]{{\includegraphics[width=0.4\textwidth, height=5cm]{../figures/conv_illustration}}}
\subfloat[{}]{{\includegraphics[width=0.4\textwidth, height=5cm]{../figures/conv_zeropad_illustration}}}
\caption[Convolutional layer illustration]{Two examples of a convolutional layers forward pass, which is entirely analogous to equation \ref{eq:fwd_multi} for fully connected layers. The convolutional layer maintains a $N$ kernels, or filters, that slides over the input taking the dot product for each step, this is the convolution of the kernel with the input. In \textbf{(a)} a $3\times3$ kernel is at the first position of the input and produces one floating point output for the $9$ pixels it sees. The kernel is a matrix of weights that are updated with backpropagation of errors. An obvious problem with \textbf{(a)} is that the kernel center cannot be placed at the edges of the image, we solve this by padding the image with zeros along the outer edges. This zero-padding is illustrated in \textbf{b} where zeros are illustrated by the dashed lines surrounding the image. The kernel then convolves over the whole image including the zeroed regions thus loosing less information. Figure copied from \citet{Dumoulin2016}}\label{fig:conv_aritmetic} 
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../figures/lenet5}
\caption[Original LeNet architecture]{The architecture \citet{Lecun1998} used when introducing convolutional layers. Each convolutional layer maintains $N$ kernels with initially randomized weights. These $N$ filters act on the same input but will extract different features from the input owing to their random initialization. The output from a convolutional layer then has size determined by equation \ref{eq:conv_out} multiplied by the number of filters $N$. Every $t$-th layer will down-sample the input, usually by a factor two. Either by striding the convolution or by MaxPooling.}\label{fig:lenet5}
\end{figure}

Originally proposed by \citet{Lecun1998} convolutional layers were used as feature extractors, i.e., to recognize and extract parts of images that could be fed to ordinary fully connected layers. The use of convolutional layers remained in partial obscurity for largely computational reasons until the rise to preeminence when Alex Krizhevsky et al. won a major image recognition contest in 2012 \cite{Krizhevsky2012} using connected GPUs (graphics processing unit). A GPU is a specialized device constructed to write data to display to a computer screen, which involves large matrix-multiplications. This property is what Krizhevsky et al. used to achieve the entrance of convolutional networks truly to the main-stage of machine learning. 

Since then, there have been major revolutions in architecture for the state-of-the-art. Inception modules showed that combinations of filters are functionally the same as ones with larger kernels, yet maintain fewer parameters \cite{Szegedy2014}. Residual networks used skip connections, passing the original data forward to avoid vanishing gradients, and batch normalization (discussed in section \ref{sec:batchnorm}). In this thesis, however, the number of classes and amount of data is still far less complex than the cases where these improvements have really shown their worth\footnote{The AT-TPC produces data on the order of $10^5$ samples and $10^0$ classes while inception-net and residual nets deal with datasets on the order of millions of samples and $10^3$ classes}.

\subsubsection{A small digression on GPUs}
Usually, these devices are used in expensive video operations such as those required for visual effects and video games. They are specialized in processing large matrix operations which is exactly the kind of computational resource neural networks profits from. The major bottle-neck they had to solve was the problem of memory; at the time a GPU only had about $3 GB$ of memory. They were however well equipped to communicate without writing to the central system memory, so the authors ended up implementing an impressive load-sharing paradigm \cite{Krizhevsky2012}. Modern consumer-grade GPUs have up to $10 GB$ of memory and have more advanced sharing protocols further cementing them as ubiquitous in machine learning research. In this thesis, all models were run on high-end consumer-grade GPUs hosted by the AI-HUB computational resource at UIO.
