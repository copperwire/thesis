
\section{On information}\label{sec:information}

Up until now, we have been dealing with predicting a real-valued outcome. However,  this is not always the goal. A prevalent task in machine learning is predicting the odds, or probability, of some event or confidence in a classification. The term classification is a general term in machine learning literature which defines a task where the goal is to predict a discrete outcome. Examples include predicting the species of an animal, or thermodynamic state of a system. Transitioning the type of goal our model has necessitates some new terminology. In this section, we will briefly touch on some fundamental concepts in information theory needed to construct models that perform a classification task.

One of the fundamental sizes in information theory is the amount of chaos in a process.  As well as how much one needs to know to characterize the same process.  A process can be the toss of a coin, or roll of a dice. These concepts tie into well-known phenomena to physicists from statistical and thermal physics. As a quick refresher, we mention that more random processes possess more information in this formalism, i.e., a rolling die has more information than a spinning coin. We define the information of an event in the usual way as 

\begin{equation}
I := -\log(p(\boldsymbol{x})),
\end{equation} 

\noindent where $p(\boldsymbol{x})$ is the probability of a given event $\boldsymbol{x}$ occurring. One of the quantities that have extensive applications is the expectation over information, known as the entropy of a system. We define the entropy as just that, the expectation over the information:

\begin{equation}
H(p(\boldsymbol{x})):= -\langle I(\boldsymbol{x}) \rangle_{p(\boldsymbol{x})}.
\end{equation}

\noindent Depending on the choice of the base of the logarithm, this functional has different names. However, the interpretation is mostly the same. Entropy measures the degree of randomness in the system. The base two entropy, known as the Shannon entropy, describes how many bits we need to fully describe the process underlying $p(\boldsymbol{x})$. 

In machine learning, or indeed many other applications of modeling, we wish to encode a process with a model. We can then measure the amount of bits (or other units of information) it takes to encode the underlying process, $p(\hat{y} | \boldsymbol{x})$, with a model distribution $q(y| \boldsymbol{x}; \theta)$. We re-iterate that in this thesis, we will, in general use the semi-colon notation to denote model parameters. The measure of information lost by encoding the original process with a model is called the cross-entropy and is defined as

\begin{equation}
H(p, q) := - \sum_{\boldsymbol{x}} p(\boldsymbol{x})\log(q(\boldsymbol{x}; \theta)).
\end{equation}

\noindent With the cross-entropy, we have arrived at a way to measure information lost by using the model $q$. This means we can use the cross-entropy as a tool to optimize the model parameters. We begin by simply considering a binary outcome $y_i$ as a function of a state $\boldsymbol{x}_i$ and define the Maximum Likelihood Estimate (MLE) as the probability of seeing the data given our model and parameters. Let the data be a set consisting of tuples\footnote{a tuple is a data structure consisting of an ordered set of different elements. It differs from a matrix in that the constituent elements need not be of the same dimension.}, $s_i = (\boldsymbol{x}_i, \hat{y}_i)$, and denote that set as $S = \{s_i\}$ then the likelihood of our model is defined as 

\begin{equation}\label{eq:likelihood}
p(S | \theta) := \prod_i q(\boldsymbol{x}_i; \theta)^{\hat{y}_i} - (1-q(\boldsymbol{x}_i; \theta))^{1-\hat{y}_i}.
\end{equation}

\noindent We want to maximize this functional with respect to the parameters $\theta$. The product sum is problematic for the optimization, as its gradient will likely vanish with increased terms . To circumvent this, we take the logarithm of the likelihood and define the log-likelihood. Since we define the likelihood as a maximization problem, we define the negative log-likelihood as the corresponding minimization problem. Optimizing the log-likelihood yields the same optimum as for the likelihood as the logarithmic function is monotonic \footnote{it can be shown that for optimization purposes any monotonic function can be used, the logarithm turns out to be practical for handling the product sum and exponents.}
\begin{equation}\label{eq:mle}
\mathcal{C}(\boldsymbol{x}, y, \theta) = -\log(p(S | \theta)) = -\sum_i y_i\log(q(\boldsymbol{x}_i; \theta)) + (1-y_i)(q(\boldsymbol{x}_i; \theta)).
\end{equation}

\noindent Where we observe this is simply the cross-entropy for the binary case. The optimization problem is then 

\begin{equation}
\theta^* = \argmin_\theta \mathcal{C}(\boldsymbol{x}, \hat{y}, \theta ).
\end{equation}

\noindent This formulation of the MLE for binary classification can be extended to the case of linear regression where one shows the mean squared error is the functional to optimize for. The MLE  is for most models notÂ analytically solvable, and so in machine learning the solution of these optimization problems is found by gradient descent. Gradient descent is discussed in some detail in section \ref{sec:gd}. The first place we find that we need iterative methods is in the section immediately following this, where we discuss the second principal machine learning algorithm; logistic regression.