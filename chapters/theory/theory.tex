\chapter{Deep learning theory}\label{ch:ml}

Deep learning describes the subfield of machine learning which uses neural network-based algorithms. A neural network is a computational structure that generalizes the logistic and linear regression framework introduced in the previous chapter.

This thesis explores to what degree we can extract salient information about different nuclear reactions occurring in an active target time projection chamber (AT-TPC) experiment using modern deep learning methods. To achieve this, we explore the latent spaces of various deep learning algorithms. Mainly we employ the deep recurrent attentive writer (DRAW) algorithm \cite{Gregor2015} and variations of a traditional autoencoder, in conjunction with logistic regression and various clustering algorithms. Both of the deep learning architectures are first used to explore our ability to create class-separating compressions. We then modify those algorithms to investigate the feasibility of clustering techniques on the compressed space. 

Building up to the algorithms used for analysis in this thesis, we start by introducing the underlying neural network framework in section \ref{sec:ANN}. Moving forward, we discuss the algorithm for optimizing such networks with a gradient descent procedure. We discuss gradient descent in some detail in section \ref{sec:gd}. Moreover, we also consider the fundamental constituents of the neural network: the layer structure and the activation function. To round out the chapter, we discuss ways to modify both the layer structure and activation function to fit different kinds of data.

A neural network consists of multiple layers that transform the input such that it can be used in e.g., classification. Each layer is traditionally expressed as an inner product, similar to how we formulated linear regression in equation \ref{eq:linreg}, stacked on top of each other. The layers may also be some more complex transformation. We will touch different formulations for neural networks used image analysis, and for time-series.  Between each layer, a non-linear function is applied to enhance the expressibility of the model further. Choosing the correct nonlinearity has been a subject of debate for the last decade in deep learning literature.

Lastly, we introduce the analysis pipelines used in this work. We utilize different models in conjunction with choice tools for the measurement of performance and a framework for hyper-parameter tuning. 