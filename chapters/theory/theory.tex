\chapter{Deep learning theory}\label{ch:ml}
\section{Introduction}

Deep learning is the region of machine learning that concerned with neural network based algorithms. A neural network is a computational structure which generalizes the logistic and linear regression framework introduced in the previous chapter to include many intermediary computations before making a prediction.

The research question being explored in this thesis is to what degree we can extract salient information about different nuclear reactions occurring in a AT-TPC (active target time projection chamber) experiment using modern deep learning methods. To achieve this we employ the DRAW algorithm (\cite{Gregor2015}) and variations of a traditional autoencoder in conjunction with logistic regression and various clustering algorithms. Both of these architectures are firstly used to explore our ability to create class-separating compressions. Secondly we modify those algorithms slightly to investigate the feasibility of applying clustering techniques on the compressed space. 

Building up to the algorithms used for analysis in this thesis we start by introducing the basic neural framework in section \ref{sec:ANN}. Moving forward we discuss the algorithm for optimizing such networks with a gradient descent procedure as discussed in section \ref{sec:gd}. We also consider the fundamental constituents of the neural network; the layer structure and the activation function. A neural network consists of multiple layers that transform the input in a way such that it can be used in e.g. classification. This layer can be an inner product like how we formulate linear regression in equation \ref{eq:linreg} stacked on top of each-other, or something more complex considering the structure of the data. Images, time-series and other data types warrant different models, all of which we touch on briefly. Between each of these layers a non-linear function is applied to further enhance the expressibility of the model, the choice of which has been a subject of debate for the last decade in deep learning literature.

Lastly we introduce the analysis pipelines used in this work. We utilize different models in conjunction with choice tools for the measurement of performance and a framework for hyper-parameter tuning. 