\chapter{Deep learning theory}\label{ch:ml}
\section{Introduction}

Deep learning describes the subfield of machine learning which uses neural network based algorithms. A neural network is a computational structure which generalizes the logistic and linear regression framework introduced in the previous chapter.


This thesis explores to what degree we can extract salient information about different nuclear reactions occurring in an AT-TPC (active target time projection chamber) experiment using modern deep learning methods. To achieve this we explore the latent spaces of various deep learning algorithms. Particularly we employ the DRAW (deep recurrent attentive writer) algorithm (\cite{Gregor2015}) and variations of a traditional autoencoder, in conjunction with logistic regression and various clustering algorithms. Both of the deep learning architectures are first used to explore our ability to create class-separating compressions. We then modify those algorithms to investigate the feasibility of clustering techniques on the compressed space. 

Building up to the algorithms used for analysis in this thesis, we start by introducing the basic neural network framework in section \ref{sec:ANN}. Moving forward we discuss the algorithm for optimizing such networks with a gradient descent procedure as discussed in section \ref{sec:gd}. We also consider the fundamental constituents of the neural network: the layer structure and the activation function. To round out the chapter we discuss ways to modify both the layer structure and activation function to fit different kinds of data.

A neural network consists of multiple layers that transform the input such that it can be used in e.g. classification. Each layer is traditionally expressed as an inner product, like how we formulated linear regression in equation \ref{eq:linreg}, stacked on top of each-other. The layers may also be some more complex transformation, considering the structure of the data. Images, time-series and other data structures warrant different models, all of which we touch on briefly in this chapter. Between each of these layers a non-linear function is applied to further enhance the expressibility of the model, the choice of which has been a subject of debate for the last decade in deep learning literature.

Lastly we introduce the analysis pipelines used in this work. We utilize different models in conjunction with choice tools for the measurement of performance and a framework for hyper-parameter tuning. 