\chapter{Deep learning theory}\label{ch:ml}
\section{Introduction}

Deep learning describes the region of machine learning which uses neural network based algorithms. A neural network is a computational structure which generalizes the logistic and linear regression framework introduced in the previous chapter to include many intermediary computations before making a prediction.

The research question being explored in this thesis is to what degree we can extract salient information about different nuclear reactions occurring in a AT-TPC (active target time projection chamber) experiment using modern deep learning methods. To achieve this we employ the DRAW algorithm (\cite{Gregor2015}) and variations of a traditional autoencoder in conjunction with logistic regression and various clustering algorithms. Both of these architectures are firstly used to explore our ability to create class-separating compressions. Secondly we modify those algorithms slightly to investigate the feasibility of applying clustering techniques on the compressed space. 

Building up to the algorithms used for analysis in this thesis, we start by introducing the basic neural framework in section \ref{sec:ANN}. Moving forward we discuss the algorithm for optimizing such networks with a gradient descent procedure as discussed in section \ref{sec:gd}. We also consider the fundamental constituents of the neural network; the layer structure and the activation function. A neural network consists of multiple layers that transform the input in a way such that it can be used in e.g. classification. Each layer is traditionally expressed as an inner product, like how we formulated linear regression in equation \ref{eq:linreg}, stacked on top of each-other. The layers may also be some more complex transformation, considering the structure of the data. Images, time-series and other data structures warrant different models, all of which we touch on briefly in this chapter. Between each of these layers a non-linear function is applied to further enhance the expressibility of the model, the choice of which has been a subject of debate for the last decade in deep learning literature.

Lastly we introduce the analysis pipelines used in this work. We utilize different models in conjunction with choice tools for the measurement of performance and a framework for hyper-parameter tuning. 