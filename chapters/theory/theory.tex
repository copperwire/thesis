\chapter{Deep learning theory}\label{ch:ml}

Deep learning describes the subfield of machine learning which uses neural network-based algorithms. A neural network is a computational structure that generalizes the logistic and linear regression framework introduced in the previous chapter.

This thesis explores to what degree we can extract salient information about different nuclear reactions occurring in an active target time projection chamber (AT-TPC) experiment using modern deep learning methods. To achieve this, we explore the latent spaces of various deep learning algorithms. A latent space describes a compressed representation of the input. Mainly we employ the deep recurrent attentive writer (DRAW) algorithm \cite{Gregor2015} and variations of a traditional autoencoder, in conjunction with logistic regression and various clustering algorithms. Both of the deep learning architectures are first used to explore our ability to create class-separating compressions. We then modify those algorithms to investigate the feasibility of clustering techniques on the compressed space. 

Building up to the algorithms used for the analysis in this thesis, we start by introducing the underlying neural network framework in section \ref{sec:ANN}. Moving forward, we discuss the algorithm for optimizing such networks with a gradient descent procedure. We discuss gradient descent in some detail in section \ref{sec:gd}. Moreover, we also consider the fundamental constituents of the neural network: the layer structure and the activation function. To round out the chapter, we discuss ways to modify both the layer structure and activation function to fit different kinds of data.

A neural network consists of multiple layers that transform the input such that it can be used in, e.g., classification. Each layer is traditionally expressed with one or more matrix multiplications, similar to how we formulated linear regression in equation \ref{eq:linreg}, stacked on top of each other. The layers may also be formulated with a more complex transformation. We will touch different formulations for neural networks used image analysis, and for time-series in the latter part of this chapter.  Between each layer, a non-linear function is applied to enhance the expressibility of the model further. Choosing the correct nonlinearity has been a subject of debate for the last decade in deep learning literature.

This chapter serves to build an understanding of the constituent parts of the algorithms implemented for this thesis. In the next chapter, we will be considering the formalism needed for their implementation. Details on the implementation are subsequently presented in chapter \ref{ch:methods}.