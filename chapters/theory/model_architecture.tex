% !TEX spellckeck=en_GB

\chapter{Neural architectures}\label{ch:architectures}

The research question explored in this thesis necessitates two separate architectures. In the case where we have access to some labelled data, the pipeline is to train an unsupervised algorithm to learn the distribution over the events. We then train a logistic regression classifier on the labelled events with their latent representations as input. Learning the data distribution is done with an autoencoder network, then the informative representation ins the compressed latent space of the model This regime is semi-supervised as most of the training time is spent trying to learn an informative compression over the data-distribution. Additionally, this approach seeks to lessen the probability of overfitting by using a very simple model as the classifier on the compressed representation. This is the \textit{classification} scheme.

 Access to labelled data is, however, not guaranteed. Labelling data requires a very well determined system with very disparate reaction events. In the ${}^{46}$Ar AT-TPC experiment, the proton and carbon products are different enough to produce visually distinct tracks, but this is not generally the case. Having access to a fully unsupervised method of separating classes of events can then be hugely beneficial to researchers. In the event where we do not have access to labelled data, we have to discover emergent clusterings without knowledge about the class distribution. In this case, we still use an autoencoder model, but we make different demands of the latent space. This is the \textit{clustering} scheme. 

\subsection{Classification}
To perform semi-supervised classification, we will leverage the autoencoder to gain information about the event distribution from the volume of unsupervised data. We summarize the modelling pipeline for the classification task in the following three steps: 

\begin{enumerate}
\item Train Autoencoder end-to-end on full data with a select regularization on the latent space until converged or it starts to overfit. 
\item Use the encoder to produce latent representations of the labelled data 
\item Train a logistic regression model using the latent representations of the labelled data 
\end{enumerate}

We will determine the best autoencoder architecture for each dataset listed in section \ref{sec:data}. The best autoencoder is measured by performance in identifying separate classes by the logistic regression model on a test-set of data. 

\subsection{Clustering}

In the case where labelled data is not available, we must turn to unsupervised methods. For the present work, we implemented two clustering algorithms based on autoencoding neural networks: the deep convolutional embedded clustering (DCEC) and the mixture of autoencoders (MIXAE) algorithm. 

The MIXAE is trained end-to-end on the individual AT-TPC datasets and subsequently evaluated by the clustering metrics presented in section \ref{sec:unsupervised_perf}.

The DCEC modelling pipeline has two distinct steps: first, we pre-train the convolutional autoencoder end-to-end on event data before applying the clustering layer loss. Recall from section \ref{sec:deep_clustering} that the clustering loss trains the soft assignments based on the model confidence. Training the DCEC algorithm then follows the following pipeline: 

\begin{enumerate}
\item Train autoencoder end-to-end on the full dataset without regularizing the latent space. 
\item Compute latent representations of the full dataset
\item Determine initial centroids from a K-means fit of the latent representations of the full dataset
\item Train the autoencoder end-to-end on the full dataset with an added regularization of the soft cluster assignments to the target distribution of pseudo labels.
\end{enumerate}

To find the best performing model, we will search over autoencoder architectures and will select the highest performing model by its performance on the labelled subset of the datasets.

\subsection{Pre-trained networks}

Following the precedent of \citet{Kuchera2019}, we will consider representations of our events through the lens of a pre-trained network. In the Machine Learning community, it is not uncommon to publish packaged models with fitted parameters from image recognition contests. These models are trained on datasets with millions of images and classify between hundreds of distinct classes; one such is the imagenet dataset. In their work, \citet{Kuchera2019} use the VGG16 architecture trained on imagenet to classify AT-TPC events very successfully.  We will build on the understanding of using these pre-trained networks in event classification by using VGG16 as an element in the end-to-end training of autoencoders. 

The VGG16 network is one of six analogous networks proposed by \citet{Simonyan2014}, and they were runners up in the ISLVRC(ImageNet large scale visual recognition competition) of 2014 \cite{Russakovsky2015}. The network architectures are fairly simple; for VGG16, there are sixteen layers in the network. The first thirteen of which are convolutional layers with exclusively $3 \times 3$ kernels. The choice of the kernel size is based on the fact that a stacked $3 \times 3$ is equivalent to larger kernels in terms of the receptive field of the output. Three $3 \times 3$ kernels with stride $1$ have a $7 \times 7$ receptive field, but the larger kernel has $81\%$ more parameters and only one non-linearity \cite{Simonyan2014}. Stacking the smaller kernels then contributes to a lower computational cost. Additionally, there is a regularizing effect from the lowered number of parameters and increased explanatory power from the additional non-linearities. The full architecture is detailed in appendix \ref{tab:vgg}.

We include the pre-trained VGG16 network in the autoencoder architectures in one of the three possible configurations. The pre-trained network can either:

\begin{enumerate}
\item Have their parameters fixed, thus creating a new representation of the input in terms of this particular model. In this way, the autoencoder does not reconstruct from the image $x$ but rather from the representation $VGG16(x)$. The decoder is here not a mirror of the encoder.
\item Have their parameters be randomly initialized. In other words, we can use the architecture of the network but not the pre-trained weights. This configuration is just a normal autoencoder, with a mirrored encoder-decoder pair. 
\end{enumerate}

We choose the first configuration for the interface between the autoencoder models and the VGG16 model. 

Additionally, we compare the autoencoder models with a baseline VGG16 model on the \textit{classification} and \textit{clustering} tasks. We treat the output from the convolutional layers of the network as a latent representation of the data. For the \textit{classification} task, we train a logistic regression classifier on the latent representations and the targets. Similarly, for the \textit{clustering} task, we feed the latent samples to a K-means algorithm. 