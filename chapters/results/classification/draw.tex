\section{Deep Recurrent Attentive Writer }

From the previous section on the convolutional autoencoder, it is clear that we can encode class information in autoencoder latent spaces. It is then interesting to investigate whether we can improve the separation in other ways. In this section, we demonstrate the results from a recurrent model: the deep recurrent attentive writer (DRAW). We discuss the DRAW algorithm in section \ref{sec:draw}.

We begin by considering the semi-supervised classification results. Given the recurrent sampled generated by the DRAW algorithm, we compute classification performance using a flattened representation of the sequence of latent samples. As the model produces $T$ samples from the latent space, we concatenate these, which then acts as input to a logistic regression classifier.

Additionally, we investigate the performance as a function of the number of latent samples, and qualitatively probe the latent space by its t-SNE projection.

The hyperparameters of the algorithm were determined with a random search architecture, equivalently to how we determined the values for the convolutional autoencoder. To keep the search-space feasible, we empirically froze some of the hyperparameters pertaining to the architecture in the read-write function pairs. For the convolutional architecture we used four layers with stride $s=2$ and kernel sizes $k= [5,\, 5,\, 3,\, 3]$. For the attention parameters, we specified a glimpse size of $\delta=0.8$ and searched over the number of Gaussian filters $N$. We used a leaky rectified linear unit and applied the ADAM optimizer in all the model experiments.

The simulated and full datasets achieved optimal performance with a convolutional read/write configuration, while the filtered data showed the strongest performance with attention parameters. For the filtered data the search yielded filter values $N_{read} = 15$ and $N_{write}=20$. Moreover, for the full and simulated data, the optimal value for the number of convolutional filters was $8$ per layer for all layers. The remainder of the hyperparameters are presented in table \ref{tab:best_draw_hyperparams}.


\begin{table}
\centering
\caption{Hyperparameters that yielded the optimal performance on the semi-supervised task for the DRAW algorithm}\label{tab:best_draw_hyperparams}
\begin{tabular}{llll}
\toprule
Hyperparameter & Simulated & Filtered & Full \\
\midrule
\multicolumn{4}{l}{Recurrent parameters: } \\
\midrule
$Dim(\text{encoder})$ & $128$ & $512$ & $256$ \\
$Dim(\text{decoder})$ & $64$ & $512$ & $256$ \\
\midrule
\multicolumn{4}{l}{Network parameters: } \\
\midrule
Latent type & MMD & None & MMD  \\
Latent dimension & $100$ & $100$ & $10$ \\
$\beta$ & $10$ & None $100$\\
Batchnorm & False & False & True \\
\midrule
\multicolumn{3}{l}{Optimizer parameters: } \\
\midrule
$\eta$ & $\num{1e-3}$ & $\num{1e-5}$ & $\num{1e-2}$ \\
$\beta_1$ & $0.92$ & $0.94$ & $0.81$ \\
$\beta_2$ & $0.99$ & $0.99$ & $0.99$ \\
\bottomrule
\end{tabular}
\end{table}

We begin by considering the $f1$ scores of the logistic regression classifier on the latent samples. These scores are included in table \ref{tab:draw_clf}, where we note that there seems to be no large deviation from the non-sequential autoencoder. 

\begin{table}
\centering
\caption{Logistic regression classifier performance on the latent space of the DRAW algorithm.}\label{tab:draw_clf}
\input{plots/draw_clf_table.tex}
\end{table}

Additionally, we wish to characterize the latent space by how many latent samples it takes to achieve this optimal performance. We present these performance records in figure \ref{fig:draw_nsamples}. 

\begin{figure}
\includegraphics[width=0.8\textwidth, height=6.5cm]{plots/ac_draw_n_samples.pdf}
\caption[Semi supervised classification with DRAW]{Performance of the logistic regression algorithm on the three datasets as a function of number of latent samples. The latent samples are produced with the DRAW algorithm using the hyperparameters presented in table \ref{tab:best_draw_hyperparams}}\label{fig:draw_nsamples}
\end{figure}


Lastly, we wish to describe the latent space in some detail. Like the VGG16 latent space and the non-sequential convolutional autoencoder we project the latent space to a low-dimensional space. The projection is made with the t-SNE algorithm, and the results are presented in figure \ref{fig:draw_tsne}


\begin{figure}
\includegraphics[width=\textwidth, height=7cm]{plots/ac_tsne_draw.pdf}
\caption[t-SNE projection of the DRAW latent space]{t-SNE projection of the DRAW latent space.  The latent samples are produced with the DRAW algorithm using the hyperparameters presented in table \ref{tab:best_draw_hyperparams}}
\label{fig:draw_tsne}
\end{figure}
