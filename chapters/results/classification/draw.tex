\section{Deep Recurrent Attentive Writer }

From the previous section on the convolutional autoencoder it is clear that we can encode class information latent spaces. The remaining question is then whether we can cluster these representations, or if we can improve the separation in other ways. One way of discovering the latter is with a recurrent model like DRAW (deep recurrent attentive writer) as discussed in section \ref{sec:draw}. We discuss the former in the next chapter, which focuses entirely on the clustering of AT-TPC events.

In the same manner as for the convolutional autoencoder this section begins by considering the semi-supervised classification results using a flattened representation of the sequence of latent samples. To build on those results we also investigate the performance as a function of latent samples and how well the latent samples are clumped.

 