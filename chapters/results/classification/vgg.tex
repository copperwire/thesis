\section{Classification using a pre-trained model}

As outlined in chapter \ref{ch:architectures}, the pre-trained VGG16 network will serve as the base-line comparison for this work. We chose VGG16 as it has demonstrated successful performance on labelled AT-TPC data from the $^{46}Ar$ experiment.(\cite{Kuchera2019}). For each labelled dataset listed in section \ref{sec:data}, a logistic regression model was fit to the respective VGG16-representation. To estimate the variability in the result a K-fold cross validation approach was taken, with $K=5$. We report test-$f1$ scores for each class and average for the classification. The results are listed in table \ref{tab:vgg_results}

\begin{table}
\centering
\caption[VGG classification results]{Logistic regression classification results using the VGG16 representation of the labelled data listed in section \ref{sec:data}. The error is given as the standard deviation in the $f1$ score over the $K=5$ folds of cross validation.}\label{tab:vgg_results}
\begin{tabular}{lllll}
\toprule
{} & Proton & Carbon & Other & All \\
\midrule
Simulated &  $\underset{\num{+- 1.014e-03 }  }{\num{ 0.999 } }$ &  $\underset{\num{+- 1.029e-03 }  }{\num{ 0.999 } }$ &  N/A &  $\underset{\num{+- 1.022e-03 }  }{\num{ 0.999 } }$ \\
Filtered  &  $\underset{\num{+- 5.108e-02 }  }{\num{ 0.918 } }$ &  $\underset{\num{+- 4.267e-02 }  }{\num{ 0.69 } }$ &  $\underset{\num{+- 2.359e-02 }  }{\num{ 0.908 } }$ &  $\underset{\num{+- 3.911e-02 }  }{\num{ 0.839 } }$ \\
Full      &  $\underset{\num{+- 4.653e-02 }  }{\num{ 0.84 } }$ &  $\underset{\num{+- 4.860e-02 }  }{\num{ 0.668 } }$ &  $\underset{\num{+- 1.730e-02 }  }{\num{ 0.89 } }$ &  $\underset{\num{+- 3.748e-02 }  }{\num{ 0.799 } }$ \\
\bottomrule
\end{tabular}
\end{table}

 Additionally, the scarceness of labelled data begs the question of how many labelled samples is needed to achieve reliable classification. To estimate this relationship, we sample increasing subsets of the labelled data, each containing the previously selected data. For each selection, a logistic regression model is fit, and a $f1$ score is computed. This procedure is sensitive data selection effects, and so a variability estimate is computed by running this procedure $N=100$ times. We report the mean and standard deviation for each dataset. The result of this analysis is shown in figure \ref{fig:vgg_n_samples}

\begin{figure}
\centering
\includegraphics[width=0.8    \textwidth, height=6.5cm]{plots/vgg_n_samples.pdf}
\caption[VGG16 performance on labelled subsets]{VGG16 performance on increasing subsets of labelled data. The error-bars represent the $\pm 1\sigma$ interval from the variability in the selection of subsets. The y axis $f1$ score is computed as the unweighted average of the sample classes. }\label{fig:vgg_n_samples}
\end{figure}

For comparison, we also explore visualizations of the latent space of each of the models in this thesis. The latent spaces are all however in high dimensional spaces, and so we utilize a combination of a linear mapping along axes of variation (PCA) and stochastic mapping via a manifold (t-SNE). The latter renders the axes completely uninterpretable as well as making relative distances incomparable (\cite{VanDerMaaten2008}). Distance in t-SNE space is still useful, as we can still get indications of class belonging in the latent space from the visualization. The principal component in the t-SNE projection is the perplexity of the model, essentially controlling how many neighbours the algorithms consider, recommended values lie between $perp=5$ and $perp=50$ (\cite{VanDerMaaten2008}). We chose a perplexity value of $perp=15$ for all the visualizations. The latent space of the pre-trained VGG16 model is shown in figure \ref{fig:vgg_tsne} and demonstrates an evident separation of the proton class with the carbon and "other" classes. 

\begin{figure}
\centering
\includegraphics[width=\textwidth, height=7cm]{plots/vgg_tsne.pdf}
\caption[VGG16 latent visualization]{Visualization of the latent space from the VGG16 model on the three different data-sets. There is very little mixing in general between the proton class and the other two for all datasets. While the carbon and other categories seem to be mixed for the full and filtered experimental data. The axes have arbitrary non-informative units.}\label{fig:vgg_tsne}
\end{figure}