\section{Deep learning algorithms}

All the models in this thesis are implemented in the python programming language using the TensorFlow library for deep learning. All the models are open source and can be found in a github repository \url{https://github.com/ATTPC/VAE-event-classification}. In this section we will be detailing the general framework that the models have been built on. The structure is straightforward, and consists of a model class which implements shared functionalities between models. Two subclasses are implemented, one for the sequential DRAW model (discussed in section \ref{sec:draw}) and one for the non-sequential convolutional autoencoder \ref{sec:autoencoder}. We also define helper-classes to perform hyperparameter searches, and to manage mini-batches of data. Throughout the thesis we follow the convention that classes are named in the \lstinline{CamelCase} style, and functions and methods of classes in the \lstinline{snake_case} style. The DCEC (deep convolutional embedded clustering) algorithm is instantiated as a convolutional autoencoder, while the MIXAE (mixture of autoencoders) algorithm is constructed using the convolutional autoencoder class internally. 

The model classes implement two main functions: \lstinline{compute_gradients}, and \lstinline{compile_model}, which call the model specific functions that constructs the computational graph, and subsequently computes the gradients. The gradient computation is done through a TensorFlow \lstinline{optimizer} class, which defines the operations needed to update the weights. 

The \lstinline{LatentModel} class contains the framework and functions used for common training operations. In the initialization of the class it mostly defines \lstinline{self} assignments but two calls are worth notice as we explicitly clean the graph before any operations are defined. Secondly an iteration is made through a configuration dictionary to define class variables pertinent to the current experiment. The configuration explicitly defines the type of latent loss (discussed in section \ref{sec:latent}) to be used for the experiment. As well as whether or not to restore the weights of a previous run from a directory, this directory is supplied to the \lstinline{train} method of \lstinline{LatentModel} class. 

% \begin{figure}[H]
% \lstinputlisting[firstline=12, lastline=51, language=iPython]{../../../../VAE-event-classification/src/model.py}
% \caption{The definition of the \lstinline{LatentModel} class. It takes a dataset, latent variable dimension, latent loss weight $\beta$ as well as a configuration dictionary. The $\beta$ parameter should not be confused with the momentum term in gradient descent. The configuration explicitly defines the type of latent loss (discussed in section \ref{sec:latent}) to be used for the experiment.}\label{code:defmodel}
% \end{figure}

After initialization and before training the subclasses of \lstinline{LatentModel} needs to construct the computational graph defining the forward pass. As well as the pertinent operations, these include the loss components, the latent space sample and the backwards gradient descent pass. This is done via a wrapper function \lstinline{compile_model} defined in \lstinline{LatentModel} that takes two dictionaries for the graph and loss configuration. They are subclass specific and will be elaborated on later in sections \ref{sec:convae_implement} and \ref{sec:draw_implement}. The method also sets the \lstinline{compiled} flag to \lstinline{True} which is a prerequisite for the \lstinline{compute_gradients} method.

% \begin{figure}
% \lstinputlisting[firstline=51, lastline=74, language=iPython]{../../../../VAE-event-classification/src/model.py}
% \caption{Code showing the compile function in the model class. The \lstinline{LatentModel} class does not implement the functions called here, they are model specific and the class will raise a \lstinline{c++} style \lstinline{abstract class} error if one tries to call this method directly without a subclass.}\label{code:compile}
% \end{figure}

 When the model is compiled the gradients can be computed and the fetch-object for the losses prepared. This general setup is entirely analogous to what was included in the script in listing \ref{code:tf_script} with some small additions. To avoid  the problem of exploding gradients we employ the common trick of clipping the gradients by their $L_2$ norm. This is particularly useful for experiments with $ReLU$ activations. The procedure is implemented in the method \lstinline{compute_gradients}. The fetch object contains the loss components, the backwards pass operation as well as the latent sample(s) and decoder state(s). This list of operations (defining a return value for the graph) is fed to a session object for execution at train time or for inference. The runtime philosophy is that a TensorFlow \lstinline{op} is not run before the graph gets notice that something that depends on that \lstinline{op} is being computed. In the same vein as the \lstinline{compile_model} method the \lstinline{compute_gradients} method sets the flag \lstinline{grad_op} to \lstinline{True} when it is through. 

% \begin{figure} 
% \lstinputlisting[firstline=74, lastline=128, language=iPython]{../../../../VAE-event-classification/src/model.py}
% \caption{Code showing the computation required for the backwards pass of the algorithm. We utilize gradient clipping to to prevent exploding gradients, though this has been known to somewhat slow down convergence.}\label{code:compute}
% \end{figure}

The training procedure is implemented in the \lstinline{train} method which handles both checkpointing of the model to a file, logging of loss values and the training procedure itself. As discussed in section \ref{sec:gd} we use the adam mini-batch gradient descent procedure. The \lstinline{train} method also contains the code to run the pre-training required for the clustering algorithm described in section \ref{sec:dc}. Which uses an off-the-shelf version of the K-means algorithm (\todo{SKLEARN CITATION}) to find the initial cluster locations. The main loop of the method iterates over the entire dataset and performs the optimization steps. For the clustering autoencoder an additional step is also included to update the target distribution as described in section \ref{sec:dc}
