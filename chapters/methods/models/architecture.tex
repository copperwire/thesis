\section{Deep learning algorithms}

All the models in this thesis are implemented in the python programming language using the TensorFlow library for deep learning. The code is open source and can be found in a github repository \url{https://github.com/ATTPC/VAE-event-classification}. In this section we will be detailing the framework built for the algorithms discussed in chapter \ref{ch:autoencoder}.

The architecture is straightforward, and consists of a model class \lstinline{LatentModel} that implements shared functionalities between models. Individual algorithms are then implemented as subclasses of \lstinline{LatentModel}, these are discussed in detail in the coming sections. We also define helper-classes to perform hyperparameter searches, and to manage mini-batches of data. Similarly, the clustering algorithm DCEC (deep convolutional embedded clustering) is instantiated as a convolutional autoencoder, while the MIXAE (mixture of autoencoders) algorithm is constructed using the convolutional autoencoder class internally. 

Throughout the thesis we follow the convention that classes are named in the \lstinline{CamelCase} style, and functions and methods of classes in the \lstinline{snake_case} style. 

The subclasses of \lstinline{LatentModel} implement two main functions: \lstinline{compute_gradients}, and \lstinline{compile_model}, which call the model specific functions that constructs the computational graph, and subsequently computes the gradients. The gradient computation is done through a TensorFlow \lstinline{optimizer} class, which defines the operations needed to update the weights. 

The \lstinline{LatentModel} class contains the framework and functions used for common training operations. In the initialization it iterates through a configuration dictionary, which defines class variables pertinent to the current experiment. The configuration explicitly defines the type of latent loss (discussed in section \ref{sec:latent}) to be used for the experiment, as well as whether to use the DCEC clustering loss, batch-normalization or if the model should restore the weights from a previous run. These are saved in the model state and referenced at key junctions.

% \begin{figure}[H]
% \lstinputlisting[firstline=12, lastline=51, language=iPython]{../../../../VAE-event-classification/src/model.py}
% \caption{The definition of the \lstinline{LatentModel} class. It takes a dataset, latent variable dimension, latent loss weight $\beta$ as well as a configuration dictionary. The $\beta$ parameter should not be confused with the momentum term in gradient descent. The configuration explicitly defines the type of latent loss (discussed in section \ref{sec:latent}) to be used for the experiment.}\label{code:defmodel}
% \end{figure}

After initialization, but before training the model class needs to construct the computational graph which defines the forward pass of the algorithm, as well as the loss components needed for the backwards pass. These components are wrapped in the method \lstinline{compile_model}, defined in \lstinline{LatentModel}. It takes two dictionaries as arguments specifying the graph and loss configuration. They are subclass specific and will be elaborated on later in sections \ref{sec:convae_implement} and \ref{sec:draw_implement}. But they include the specification of regularization strength and type, as well as the type of activation and loss functions. The method also sets the \lstinline{compiled} flag to \lstinline{True} which is a prerequisite for the \lstinline{compute_gradients} method.

% \begin{figure}
% \lstinputlisting[firstline=51, lastline=74, language=iPython]{../../../../VAE-event-classification/src/model.py}
% \caption{Code showing the compile function in the model class. The \lstinline{LatentModel} class does not implement the functions called here, they are model specific and the class will raise a \lstinline{c++} style \lstinline{abstract class} error if one tries to call this method directly without a subclass.}\label{code:compile}
% \end{figure}

When the model is compiled we turn to the computation of the gradients. In this thesis we use TensorFlow \lstinline{optimizer} objects to prepare the update operations needed in the training. The \lstinline{compute_gradients} method then takes an optimizer object, e.g. \lstinline{AdamOptimizer}, and its positional and keyword arguments. Inside the method the optimizer is instantiated in order to compute the gradients. Lastly, the optimizer prepares the update operations which is fed to the \lstinline{Session} object to update the weights. The model is now ready for training.


% \begin{figure} 
% \lstinputlisting[firstline=74, lastline=128, language=iPython]{../../../../VAE-event-classification/src/model.py}
% \caption{Code showing the computation required for the backwards pass of the algorithm. We utilize gradient clipping to to prevent exploding gradients, though this has been known to somewhat slow down convergence.}\label{code:compute}
% \end{figure}

The training procedure is implemented in the \lstinline{train} method of the \lstinline{LatentModel} class, and handles both checkpointing of the model to a file, logging of loss values and the training procedure itself. As discussed in section \ref{sec:gd} we use the adam mini-batch gradient descent procedure. The \lstinline{train} method also contains the code to run the pre-training required for the DCEC algorithm, described in section \ref{sec:deep_clustering}. As part of that procedure we use an off-the-shelf version of the K-means algorithm, implemented in the \lstinline{scikit-learn} package (\cite{Pedregosa2011}) to find the initial cluster locations. The main loop of the method iterates over the entire dataset and performs weight updates. For the  DCEC an additional step is also included to update the target distribution $p_{ij}$.
