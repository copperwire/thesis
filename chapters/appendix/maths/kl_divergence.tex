\chapter{Kullback-Leibler divergence of of Gaussian distributions}\label{appendix:kl_gauss}

A multivariate Gaussian distribution in $\R^n$ is defined in terms of its probability density, which is a complete analogue to its univariate formulation, 
\begin{equation}\label{eq:multi_gauss}
p(x) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x- \mu)^T\Sigma^{-1}(x-\mu)).
\end{equation}

\noindent And described in full by the mean vector $\mu$ and covariance matrix $\Sigma$. The  Kullback-Leibler divergence between two multivariate Gaussians is then given as 

\begin{align*}
D_{KL}(p_1|| p_2 ) &= \langle \log p_1 - \log p_2 \rangle_{p_1} \\
&= \langle \frac{1}{2}\log \frac{|\Sigma_2|}{|\Sigma_1|} + \frac{1}{2} (-(x- \mu_1)^T\Sigma_1^{-1}(x-\mu_1) + (x- \mu_2)^T\Sigma_2^{-1}(x-\mu_2))   \rangle.
\end{align*}

\noindent We abuse the fact that the exponential factors represent an inner-product to apply a trace operator to manipulate the sequence of operations given the trace operators invariance under cyclical permutations i.e. $tr(X^TBX) = tr(BX^TX)$. Furthermore we use the fact that the trace is a linear operator and so commutes with the expectation i.e. $E(tr(BX^TX)) = tr(B\, E(X^TX))$. We also move the logarithm of the covariance determinants outside of the expectations, 

\begin{align*}
D_{KL}(p_1|| p_2 ) &= \frac{1}{2}\log \frac{|\Sigma_2|}{|\Sigma_1|} + \frac{1}{2} \langle - tr(\Sigma^{-1}_1(x-\mu_1)^T(x-\mu_1)) + tr(\Sigma^{-1}_2(x-\mu_2)^T(x-\mu_2))\rangle \\
&= \frac{1}{2}\log \frac{|\Sigma_2|}{|\Sigma_1|} + \frac{1}{2} (- tr(\Sigma^{-1}_1\langle(x-\mu_1)^T(x-\mu_1)\rangle) + tr(\Sigma^{-1}_2\langle(x-\mu_2)^T(x-\mu_2)\rangle)).
\end{align*}

\noindent Conveniently the covariance matrix is defined by the expectation 

\begin{equation}
\Sigma := \langle (x-\mu)^T(x-\mu)\rangle, 
\end{equation}

\noindent giving an evident simplification. For the terms originating from $p_2$ we will use the definitions of the covariance matrix and the mean vector, i.e. $\mu = \langle x \rangle$ and 

\begin{align*}
\Sigma &= \langle x^Tx -2x\mu^T + \mu \mu^T\rangle \\
\Sigma &= \langle x^Tx \rangle - \mu \mu^T.
\end{align*} 

\noindent Returning to the Kullback-Leibler divergence we then have 

\begin{align*}
D_{KL}(p_1|| p_2 ) &= \frac{1}{2}\log \frac{|\Sigma_2|}{|\Sigma_1|} + \frac{1}{2} (- tr(\Sigma^{-1}_1 \Sigma_1) + tr(\Sigma^{-1}_2\langle x^Tx -2x\mu_2^T + \mu_2 \mu_2^T \rangle)) \\
&= \frac{1}{2} \left( \log \frac{|\Sigma_2|}{|\Sigma_1|} -n + tr(\Sigma^{-1}_2 (\Sigma_1 + \mu_1\mu_1^T - 2\mu_1\mu_2^T + \mu_2 \mu_2^T ))\right). 
\end{align*}

\noindent Grouping terms then gives us the final expression for the Kullback-Leibler divergence of two multivariate Gaussians

\begin{equation}\label{eq:kl_gauss}
\hspace*{-0.1in}
D_{KL}(p_1|| p_2 ) = \frac{1}{2} \left( \log \frac{|\Sigma_2|}{|\Sigma_1|} -n + tr(\Sigma^{-1}_2 \Sigma_1) + (\mu_2 - \mu_1)^T\Sigma_2^{-1}(\mu_2 - \mu_1) \right).
\end{equation}