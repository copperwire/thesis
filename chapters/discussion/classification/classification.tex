\section{Semi-supervised classification of AT-TPC events}
Recall that the first question we wish to explore in this thesis is whether training an autoencoder encodes class-information in the latent space. As a benchmark, we trained a linear model on the data-representations from a pre-trained VGG16 network. This high-performing model from the image analysis community has seen successful applications to the same experimental data; it then follows that it is also a reasonable comparison for our methods \cite{Kuchera2019}. 

\subsection{Convolutional autoencoder}
Using the \lstinline{RandomSearch} framework, we were able to find a network configuration for the convolutional autoencoder that shows very strong performance on the simulated data. With a total $f1$ score of $>0.96$, the simulated data provide another benchmark for the results in this section.

 Furthermore, we observe that for the filtered and full datasets, the autoencoder encodes a high-quality representation of the data with $f1$ scores $>0.7$. We also note that the latent space presents strong class-separability with and without a latent loss attached to the optimization. It is, however clear that the maximum mean discrepancy (MMD) regularization is preferable to the variational autoencoder objective. This indicates that sample-wide measures of regularization are more capable of encoding class-information than the point-based KL-objective. We note that this finding is in agreement with the arguments presented by \citet{Zhao}. 

 From comparing the results in table \ref{tab:clf_no_vgg} and \ref{tab:clf_freeze_vgg} to those in table \ref{tab:vgg_results} it is clear that while the autoencoder achieves strong class separation the reconstruction objective is somewhat misaligned with a class-separating representation. The misalignment can be attributed to the fact that the pre-trained VGG16 network creates latent spaces which explicitly aims to separate classes. This explicit nature is notably absent from the reconstruction, and latent objectives, of the convolutional autoencoder. Further cementing this argument is the improvement in classification performance when using the VGG16 features as input to the autoencoder. In summary, we argue that when labelled data is present, using pre-trained networks for classification of events is the recommended procedure.

\subsubsection{Constructing a salient latent space}

To understand why the MMD cost is preferable, in terms of classifier performance, to the VAE objective, we turn to a semi-supervised VAE developed by \citet{Antoran2019}. In their work, the authors show that the mapping of the latent space to an isotropic Gaussian distribution, as the Kullback-Leibler objective aims to achieve, contributes to the washing out of class information. However, they also show that it strongly encourages feature level information. They demonstrate these qualities on the MSIST handwritten digits dataset, where the feature information is described as, e.g. stroke thickness or skew when drawing a number, while class information is the more esoteric "five"-ness of all drawings of the number five. This is illustrated in figure \ref{fig:latent_traversal}, where the spaces between the class blobs generated by the $\beta-$VAE makes for a weak generative algorithm. However, for the purpose of classification or even clustering, this is strongly preferable. On the right, the natural clustering of feature information is demonstrated by the convincingly isotropic nature of the latent space distribution. The subplots under the latent distributions demonstrate reconstructions of a traversal along a latent axis, clearly showing the difference between feature- and class information. 

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth, height=2.3in]{plots/latent_traversal}
\caption[Difference between generative and discriminative latent spaces]{Demonstrating the difference of capturing class and feature information in the latent space. On the left, the $\beta$-VAE pushes the autoencoder to a representation favouring encoded class information in the latent space. On the right, the natural VAE which has a separate classification from the latent sample exhibits a tight distribution of the latent space. The sub-plots demonstrate the traversal in the output space when sampling from the latent x-axis. Figure copied from \citet{Antoran2019}.}\label{fig:latent_traversal}
\end{figure}

Indeed their semi-supervised objective improves the generative aspect of the variational autoencoder by tightening the latent distribution, i.e. achieving a density in the latent space without holes. This allows for the generation of new samples without the risk of regions in the latent prior for which the output is poorly defined.

An additional challenge attached to the latent space is the threat of mode collapse, as described in section \ref{sec:mode_collapse}. This problem is investigated in detail by \citet{Seybold2019}, where the authors propose the duelling decoder objective as a solution to this problem. The duelling decoder adds a second reconstruction term to the objective. This second reconstruction is optimized over a different representation of the data, e.g. reconstructing the edges in an image, its intensity histogram or other transformations. For applications in physics, this is a promising approach as it allows the inclusion of physical properties to the optimization. From the results in table \ref{tab:dd_clf_no_vgg} as compared to those in table \ref{tab:clf_no_vgg} there are indeed indications that the duelling decoder architecture contributes to more salient representations in the latent space. 

For future work on autoencoder based clustering and classification, the addition of a duelling-decoder objective holds promise. Our results show a marginal increase in performance when predicting the charge histogram of an event, compared to just reconstructing the two-dimensional representation of the event. Further work is required on what representations generate the most salient representations in the latent space. One such representation which we did not explore in this thesis is the Hough's transform of the data. As previously described in section \ref{sec:filtered}, the Hough's transform is a representation of the geometry in the event. Since the curvature of the track is dependent on the velocity of the electron\footnote{In the AT-TPC configuration with an applied magnetic field, the electron would experience a Lorentz force}, a precise description of the geometry would then encode important physical aspects of the event.


\subsubsection{Classifier performance}

From table \ref{tab:clf_no_vgg} it is clear that while the autoencoder architecture, \ref{item:clf_no_vgg}, is able to capture class information it does not outperform the pre-trained VGG16 in terms of the linear separability of its latent space. Additionally, the performance in terms of the number of latent samples was not improved by the autoencoder architecture either. 

Visually inspecting the latent space in figure \ref{fig:ac_tnse}, we observe the same types of structures for all architectures; local high-density areas that are strung out in the space. For the filtered and full datasets, we observe a slight degradation of proton separation compared to the pure VGG16 representation, which we confirm by the proton $f1$ scores in table \ref{tab:clf_no_vgg}. Carbon is consistently hard to separate from the amorphous "other" category, and there is no indication that the autoencoder is able to separate them better than the pure VGG16 latent already does. 

Using the VGG16 representation as an initial encoded representation, \ref{item:clf_freeze_vgg} improved performance substantially from the \ref{item:clf_no_vgg} architecture. The mean performance is close to the VGG16 performance, but the standard error is greater. In the testing regime for the n-labelled performance, the former is more important than the latter, as the testing set is held constant. Inspecting the performance as a function of n-labelled samples, we observe that the \ref{item:clf_freeze_vgg} autoencoder exhibits the same patterns of error as the pure VGG16, with very small deviations from the mean for the filtered and simulated data and an error in the second decimal for the full data.

We note that in figure \ref{fig:ac_n_labelled} and \ref{fig:vgg_ac_n_labelled} the asymptotic performance is not expected to tend to the mean represented their corresponding tables as the test set is held constant. The K-means approach is then a better estimate of the true mean of the performance on the labelled set.

The obvious question is then why the reconstruction objective does not aid in classification. To find a plausible answer, we look to a discussion on the principal component analysis (PCA) from \cite{Jolliffe1982}, where he remarks that the major axes of variation may not be the ones carrying class information. We refer to section \ref{sec:unsupervised_learning} for details on the PCA algorithm. This relates to the reconstruction objective where we posit that the reconstruction focuses the optimization on these major axes of variation, and if they do not carry the salient class-separating information, there is no reason to believe the latent space would. Contributing to this argument is the observation that adding the duelling decoder improves the classifier. In summary, we observe that the autoencoder does not improve classification compared to the pre-trained model latent space. However, there are still interesting avenues to investigate autoencoder models. In particular, the duelling decoder objective could provide an interesting link to the geometry, charge distribution or other physical properties for the latent space.

% \begin{figure}
% \centering
% \includegraphics[width=\textwidth]{plots/loss_func_shape.png}
% \caption[Illustrating differences in the shapes of the MSE and BCE-loss]{Illustrating the difference between the function shapes of binary cross-entropy and the mean squared error for the same target value $y=0.7$. We observe that though the two functions have the same minimum the landscape surrounding it is very different. Perhaps most telling is the asymmetry of the cross entropy, implicitly telling the optimizer that values above the targets are measurably worse than those below as measured by the steepness of the gradient. For the AT-TPC data this then implies that predicting higher charge values are worse than predicting lower than the target, and implication that is not physically substantiated}\label{fig:loss_func_shape}
% \end{figure}