\section{Semi-supervised classification of AT-TPC events}

To prime or discussion of the semi-supervised classification results, we briefly restate the goals for the analysis: The core task we aim to accomplish is to quantify the model performance as in terms of the available labelled data. Furthermore, we wish to characterize the latent spaces produced by the different algorithms qualitatively. The performance is contextualized by the work of \citet{Kuchera2019}, who introduced the application of pre-trained models to AT-TPC data. Lastly, the semi-supervised performance serves as a proof-of-concept for the construction of high-quality latent spaces in an AT-TPC experiment. This proof-of-concept spurred the implementation of autoencoder based algorithms for clustering of AT-TPC data.

As a benchmark, we trained a linear model on the data representations from a pre-trained VGG16 network. This high-performing model from the image analysis community has seen successful applications to the same experimental data; it then follows that it is also a reasonable comparison for our methods \cite{Kuchera2019}. 

Additionally, we explore the performance of our convolutional autoencoder models trained end-to-end on AT-TPC data. In this section we compare and contrast the pre-trained model and the autoencoder models on the aforementioned analysis objectives.

\subsection{Pre-trained networks}

From table \ref{tab:vgg_results} it is evident that the VGG16 network, even when trained on an auxiliary task, projects the AT-TPC data to a linearly separable space. In addition to the end-point f1 scores, the performance as  a function of labelled data paints a convincing picture of the latent space. This picture is painted by figure \ref{fig:vgg_n_samples}, from which we see that for the simulated data, the linear classifier performs almost perfectly when trained on $100$ labelled samples. Additionally, the variance is low for the filtered data but increases significantly for the full data. The relative difference in class occurrences may explain a part of this discrepancy. However, the major difference is in the amount of noise present in the events. An increased variability as a function of noise is an expected consequence. This indicates that pre-processing of data is an important step when employing pre-trained models to AT-TPC data. 


\subsection{Convolutional autoencoder}

Using the \lstinline{RandomSearch} framework, we were able to find a network configuration for the convolutional autoencoder (CAE) that shows solid performance on the simulated data. With a total $f1$ score of $>0.96$, the simulated data strongly indicate that the CAE will perform well for our real data.

 Furthermore, we observe that for the filtered and full datasets, the autoencoder encodes a high-quality representation of the data with $f1$ scores $>0.7$. We also note that the latent space shows strong class-separability with and without a latent loss. This means that the CAE class separability is more closely tied to the reconstruction objective than the latent prior. It is, however, clear that the maximum mean discrepancy (MMD) regularization is preferable to the variational autoencoder objective. This preference indicates that sample wide measures of regularization are more capable of encoding class-information than the point-based KL objective. This finding is in agreement with the arguments presented by \citet{Zhao}. 

 From comparing the results in table \ref{tab:clf_no_vgg} and \ref{tab:clf_freeze_vgg} to those in table \ref{tab:vgg_results} it is clear that while the autoencoder achieves strong class separation the reconstruction objective is somewhat misaligned with a class-separating representation. The misalignment can be attributed to the fact that the pre-trained VGG16 network creates latent spaces which explicitly aims to separate classes. This explicit nature is notably absent from the reconstruction, and latent objectives, of the CAE. Further cementing this argument is the improvement in classification performance when using the VGG16 features as input to the autoencoder. In summary, we argue that when labelled data is present, using pre-trained networks for classification of events is the recommended procedure.

\subsubsection{Constructing a salient latent space}

We also want to understand why the MMD cost is preferable, in terms of classifier performance, to the VAE objective. This understanding can be brought about by considering a semi-supervised VAE developed by \citet{Antoran2019}. In their work, the authors show that the mapping of the latent space to an isotropic Gaussian distribution, as the Kullback-Leibler objective aims to achieve, contributes to the washing out of class information. However, they also show that by predicting a class assignment separate from the latent sample, the KL-divergence strongly encourages feature level information. They demonstrate these qualities on the MNIST handwritten digits dataset, where the feature information is described as, e.g. stroke thickness or skew when drawing a number, while class information is the more esoteric "five"-ness of all drawings of the number five. These differences are illustrated in figure \ref{fig:latent_traversal}. On the right, the latent space and generated samples from the natural clustering algorithm. On the left, the same but for the $\beta$-VAE. The subplots under the latent distributions demonstrate reconstructions of a traversal along a latent axis, clearly showing the difference between feature- and class information. 



\begin{figure}
\centering
\includegraphics[width=0.9\textwidth, height=2.3in]{plots/latent_traversal}
\caption[Difference between generative and discriminative latent spaces]{Demonstrating the difference of capturing class and feature information in the latent space. On the left, the $\beta$-VAE pushes the autoencoder to a representation favouring encoded class information in the latent space. On the right, the natural VAE which has a separate classification from the latent sample exhibits a tight distribution of the latent space. The sub-plots demonstrate the traversal in the output space when sampling from the latent x-axis. Figure from \citet{Antoran2019}.}\label{fig:latent_traversal}
\end{figure}

Indeed their semi-supervised objective improves the generative aspect of the variational autoencoder by tightening the latent distribution, i.e. achieving a density in the latent space without holes. Tight distributions of latent samples improve the generative properties by having no regions of the prior for which the generated sample is poorly defined. 

A variation of this algorithm may apply to transfer-learning problems with AT-TPC data. The challenge presented by the application stems from the cluster assignments, as these require training with some supervision. A natural application to AT-TPC data is to pre-train this algorithm on simulated data and investigate the clustering on records from an experiment. 

\subsection{Mode collapse and the duelling decoder}

An additional challenge attached to adding a constraint on the latent space is the threat of mode collapse, as described in section \ref{sec:mode_collapse}. During the development of the autoencoder models mode collapse was an oft-occurring problem, this often coincided with the inclusion of a strongly penalized latent loss. This problem is investigated in detail by \citet{Seybold2019}, where the authors propose the duelling decoder objective as a solution to this problem. The duelling decoder adds a second reconstruction term to the objective. This second reconstruction is optimized over a different representation of the data, e.g. reconstructing the edges in an image, its intensity histogram or other transformations. For applications in physics, this is a promising approach as it allows the inclusion of physical properties to the optimization. From the results in table \ref{tab:dd_clf_no_vgg} as compared to those in table \ref{tab:clf_no_vgg} there are indications that the duelling decoder architecture contributes to more salient representations in the latent space. 

For future work on autoencoder based clustering and classification, the addition of a duelling-decoder objective holds promise. Our results show a marginal increase in performance when predicting the charge histogram of an event, compared to just reconstructing the two-dimensional representation of the event. Further work is required on what representations generate the most salient representations in the latent space. One such representation which we did not explore in this thesis is the Hough's transform of the data. As previously described in section \ref{sec:filtered}, the Hough's transform is a representation of the geometry in the event. Since the curvature of the track is dependent on the velocity of the electron\footnote{In the AT-TPC configuration with an applied magnetic field, the electron would experience a Lorentz force}, an accurate description of the geometry would then encode critical physical aspects of the event.


\section{Classifier performance}

From table \ref{tab:clf_no_vgg} it is clear that while the autoencoder architecture, \ref{item:clf_no_vgg}, is able to capture class information, it does not outperform the pre-trained VGG16 in terms of linear separability of its latent space. Adding the VGG16 representations to the autoencoder increased the performance, as we see from table \ref{tab:clf_freeze_vgg}, but did not increase it beyond the pure VGG16 classifier.  Additionally, the performance in terms of the number of latent samples was not improved by the autoencoder architecture when compared to the pre-trained model. The variance is the principal measure of improvement in the performance with low-volume labelled data, as well as the mean performance relative to the end-point mean. From figures \ref{fig:vgg_n_samples}, \ref{fig:vgg_ac_n_labelled}  and \ref{fig:ac_n_labelled} we can infer that the autoencoder algorithms do not meaningfully increase classification performance compared to the pre-trained model in terms of the number of labelled samples.

Visually inspecting the latent space in figure \ref{fig:ac_tnse}, we observe the same types of structures for all architectures; local high-density areas that are strung out in the space. For the filtered and full datasets, we observe a slight degradation of proton separation with the naive autoencoder compared to the pure VGG16 representation. This result is confirmed by the proton $f1$ scores in tables \ref{tab:clf_no_vgg}. Carbon is consistently hard to separate from the amorphous "other" category, and there is no indication that the autoencoder is able to separate them better than the pure VGG16 latent already does. It is important to note that the ${}^{46}$Ar experiment was not designed to detect carbon reactions, and so the confusion with noise sources is not necessarily a cause for concern. 

Using the VGG16 representation as an initial encoded representation, the \ref{item:clf_freeze_vgg} architecture improved performance substantially from the \ref{item:clf_no_vgg} architecture. Moreover, the autoencoder trained on the VGG16 representations showed increased performance on the proton class by lowering the variance by about $\sim 64\%$ from the pure VGG16 classifier. This is evident from table \ref{tab:clf_freeze_vgg} and \ref{tab:vgg_results}. When trained on few labelled samples the \ref{item:clf_freeze_vgg} architecture achieved comparable results to the pre-trained model. We observe from figure \ref{fig:ac_n_labelled} and \ref{fig:vgg_ac_n_labelled} that the \ref{item:clf_freeze_vgg} autoencoder exhibits the same patterns of error as the pre-trained model, with very small deviations from the mean for the filtered and simulated data and an error in the second decimal for the full data.

We note that in figure \ref{fig:ac_n_labelled} and \ref{fig:vgg_ac_n_labelled} the asymptotic performance is not expected to tend to the mean represented their corresponding tables. In the performance estimate as a function of labelled samples, the hold-out set is chosen arbitrarily and held constant, as we are more interested in variability and the shape of the curve than the end-point itself. Conversely, the K-fold approach varies the hold-out set to approximate the expected performance given changes in how we select data to train the classifier.

We then investigate why the reconstruction objective does not aid in classification. We look to a discussion on the principal component analysis (PCA) in regression analysis from \citet{Jolliffe1982}, where he remarks that the major axes of variation may not be the ones carrying the information needed for regression\footnote{The PCA algorithm is a dimensionality reduction algorithm that projects the data long smartly chosen axes of variation. We refer to section \ref{sec:unsupervised_learning} for details on the PCA algorithm.}. We posit that the reconstruction focuses the optimization on these major axes of variation, and if these do not carry the salient class-separating information, the latent space may not carry this information either. Contributing to this argument is the observation that adding the duelling decoder improves the classifier performance.

In summary, we observe that the autoencoder does not improve classification compared to the pre-trained model latent space. However, there are still interesting avenues to investigate autoencoder models. In particular, the duelling decoder objective could provide an interesting link to the geometry, charge distribution or other physical properties for the latent space.

% \begin{figure}
% \centering
% \includegraphics[width=\textwidth]{plots/loss_func_shape.png}
% \caption[Illustrating differences in the shapes of the MSE and BCE-loss]{Illustrating the difference between the function shapes of binary cross-entropy and the mean squared error for the same target value $y=0.7$. We observe that though the two functions have the same minimum the landscape surrounding it is very different. Perhaps most telling is the asymmetry of the cross entropy, implicitly telling the optimizer that values above the targets are measurably worse than those below as measured by the steepness of the gradient. For the AT-TPC data this then implies that predicting higher charge values are worse than predicting lower than the target, and implication that is not physically substantiated}\label{fig:loss_func_shape}
% \end{figure}